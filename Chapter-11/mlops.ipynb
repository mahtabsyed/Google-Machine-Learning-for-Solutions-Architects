{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# MLOps on Vertex AI\n",
    "\n",
    "In this notebook, we will build an MLOps pipeline on Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Introduction and setup\n",
    "\n",
    "In this notebook, we will build a pipeline that will perform the following steps:\n",
    "1. Custom data processing such as feature scaling, one-hot encoding, and feature engineering, in a Google Cloud Serverless Spark environment.\n",
    "2. Implement a custom training job in Vertex AI to train a custom model. In this case, our model uses the [Titanic dataset from Kaggle](https://www.kaggle.com/competitions/titanic/data) to predict the likelihood of survival of each passenger based on their associated features in the dataset.\n",
    "3. Upload the trained model to Vertex AI Model Registry.\n",
    "4. Deploy the trained model to a Vertex AI endpoint for online inference.\n",
    "\n",
    "In this initial section, we set up all of the baseline requirements to run our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc pricing](https://cloud.google.com/dataproc/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "We will use the following libraries in this notebook:\n",
    "\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview/)\n",
    "* [Google Cloud Pipeline Components (GCPC)](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --quiet --user --upgrade google-cloud-aiplatform kfp google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines (KFP)\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, Input, Output, Artifact\n",
    "\n",
    "# Google Cloud Pipeline Components (GCPC)\n",
    "from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "In this section, we define all of the constants that will be referenced throughout the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQOV9ssPWbMB"
   },
   "outputs": [],
   "source": [
    "# Core constants\n",
    "BUCKET_URI = f\"gs://{BUCKET}\"\n",
    "APPLICATION_DIR = \"mlops-titanic-app\" # Local parent directory for our pipeline resources\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\" # Local directory for training resources\n",
    "PYSPARK_DIR = \"pyspark-titanic-dir\" # Local directory for PySpark data processing resources\n",
    "APP_NAME=\"mlops-titanic\" # Base name for our pipeline application\n",
    "\n",
    "# Pipeline constants\n",
    "PIPELINE_NAME = \"mlops-titanic-pipeline\" # Name of our pipeline\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\" # (See: https://www.kubeflow.org/docs/components/pipelines/v1/overview/pipeline-root/)\n",
    "SUBNETWORK = \"default\" # Our VPC subnet name\n",
    "SUBNETWORK_URI = f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{SUBNETWORK}\" # Our VPC subnet resource identifier\n",
    "MODEL_NAME = \"mlops-titanic\" # Name of our model\n",
    "EXPERIMENT_NAME = \"aiml-sa-mlops-experiment\" # Vertex AI \"Experiment\" name for metadata tracking\n",
    "\n",
    "# Preprocessing constants\n",
    "SOURCE_DATASET = f\"{BUCKET_URI}/data/unprocessed/titanic/train.csv\" # Our raw source dataset\n",
    "PREPROCESSING_PYTHON_FILE_URI = f\"{BUCKET_URI}/code/mlops/preprocessing.py\" # GCS location of our PySpark script\n",
    "PROCESSED_DATA_URI =f\"{BUCKET_URI}/data/processed/mlops-titanic\" # Location to store the output of our data preprocessing step\n",
    "DATAPROC_RUNTIME_VERSION = \"2.1\" # (See https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions)\n",
    "# Arguments to pass to our preprocessing script:\n",
    "PREPROCESSING_ARGS = [\n",
    "    \"--source_dataset\",\n",
    "    SOURCE_DATASET,\n",
    "    \"--processed_data_path\",\n",
    "    PROCESSED_DATA_URI,\n",
    "]\n",
    "\n",
    "# Training constants\n",
    "TRAIN_REPO_NAME=f'{APP_NAME}-train' # Name of repository in which we will store our custom training image\n",
    "TRAIN_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{TRAIN_REPO_NAME}/{APP_NAME}-train:latest\"\n",
    "MODEL_URI = f\"{BUCKET_URI}/models/mlops-chapter/titanic\" # Where to store our trained model\n",
    "# Where to store our test data:\n",
    "TEST_DATA_PREFIX = \"test_data\" \n",
    "TEST_DATA_DIR = f\"{TEST_DATA_PREFIX}_dir\"\n",
    "TEST_DATA_FILE_NAME = f\"{TEST_DATA_PREFIX}.jsonl\"\n",
    "TEST_DATASET_PATH = f\"{BUCKET_URI}/{TEST_DATA_FILE_NAME}\"\n",
    "LOCAL_TEST_DATASET_PATH = f\"./{TEST_DATA_DIR}/{TEST_DATA_FILE_NAME}\"\n",
    "\n",
    "# Hyperparameters for training\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 20\n",
    "LEARNING_RATE: float = 0.001\n",
    "N_HIDDEN_LAYERS: int = 3\n",
    "N_UNITS: int = 64\n",
    "ACTIVATION_FN: str = 'relu'\n",
    "\n",
    "# Arguments to pass to our training job\n",
    "TRAINING_ARGS=[\n",
    "        \"--project_id\",\n",
    "        PROJECT_ID,\n",
    "        \"--bucket_name\",\n",
    "        BUCKET,\n",
    "        \"--processed_data_path\",\n",
    "        PROCESSED_DATA_URI,\n",
    "        \"--test_data_file_name\",\n",
    "        TEST_DATA_FILE_NAME,\n",
    "        \"--model_path\",\n",
    "        MODEL_URI,\n",
    "        \"--batch_size\",\n",
    "        str(BATCH_SIZE),\n",
    "        \"--epochs\",\n",
    "        str(EPOCHS),\n",
    "        \"--learning_rate\",\n",
    "        str(LEARNING_RATE),\n",
    "        \"--n_hidden_layers\",\n",
    "        str(N_HIDDEN_LAYERS),\n",
    "        \"--n_units\",\n",
    "        str(N_UNITS),\n",
    "        \"--activation_fn\",\n",
    "        ACTIVATION_FN,\n",
    "    ]\n",
    "\n",
    "# Worker pool spec (see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec)\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"args\": TRAINING_ARGS\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# These will be referenced in a later chapter\n",
    "EXPLANATION_PARAMS = {\n",
    "  \"sampledShapleyAttribution\": {\n",
    "    \"pathCount\": 10\n",
    "  }\n",
    "}\n",
    "\n",
    "# Serving constants\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\" # (See: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)\n",
    "ENDPOINT_NAME = \"mlops-endpoint\" # Name of endpoint on which to serve our trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local directories\n",
    "We will use the following local directories during the activities in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a source directory to save the code\n",
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR\n",
    "!mkdir -p $PYSPARK_DIR\n",
    "!mkdir -p $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source dataset \n",
    "Upload our source dataset to GCS. Our data preprocessing step in our pipeline will ingest this data from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ./data/train.csv $SOURCE_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set  project ID for  gcloud\n",
    "The following command sets our project ID for using gcloud commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vertex AI SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Private Google Access for Dataproc \n",
    "Our Serverless Spark data preprocessing job in our pipeline will run in Dataproc, which is (as Google defines) Google's \"fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.\"\n",
    "We're going to configure something called \"Private Google Access\", which allows us to interact with Google services without sending requests over the public Internet.\n",
    "\n",
    "You can learn more about Dataproc [here](https://cloud.google.com/dataproc?hl=en), and learn more about Private Google Access [here](https://cloud.google.com/vpc/docs/private-google-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK\n",
    "\n",
    "!gcloud compute networks subnets update $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access\n",
    "\n",
    "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom PySpark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5229530e16"
   },
   "source": [
    "The following code will create a file that contains the code for our custom PySpark data preprocessing job. \n",
    "\n",
    "The code initiates a Spark session, loads our raw source dataset, and then performs the following processing steps (we performed many of these steps using pandas in our feature engineering chapter earlier in this book, but in this case we will implement the steps using PySpark in Google Cloud Serverless Spark):\n",
    "\n",
    "1. Removes rows from the dataset where the target variable (\"Survived\") is missing values.\n",
    "2. Drops columns that are unlikely to affect the likelihod of surviving, such as 'PassengerId', 'Name', 'Ticket', and 'Cabin'.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS.\n",
    "\n",
    "**Note:** we could create a custom container in which to run our PySpark code on Dataproc Serverless if we had very specific dependencies that needed to be installed. However, Dataproc Serverless also provides [default runtimes](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions) that we can use, and these are fine for our needs in this activity, so all we need to do is define our code and put it into GCS so that it can be referenced by the `DataprocPySparkBatchOp` component in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $PYSPARK_DIR/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "\n",
    "# Setting up the argument parser\n",
    "parser = argparse.ArgumentParser(description='Data Preprocessing Script')\n",
    "parser.add_argument('--source_dataset', type=str, help='Path to the source dataset')\n",
    "parser.add_argument('--processed_data_path', type=str, help='Path to save the output data')\n",
    "\n",
    "# Parsing the arguments\n",
    "args = parser.parse_args()\n",
    "source_dataset = args.source_dataset\n",
    "processed_data_path = args.processed_data_path\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "titanic = spark.read.csv(args.source_dataset, header=True, inferSchema=True)\n",
    "\n",
    "# Remove rows where 'Survived' is missing\n",
    "titanic = titanic.filter(titanic.Survived.isNotNull())\n",
    "\n",
    "# Drop irrelevant columns\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket', 'Cabin')\n",
    "\n",
    "# Fill missing values\n",
    "def calculate_median(column_name):\n",
    "    return titanic.filter(col(column_name).isNotNull()).approxQuantile(column_name, [0.5], 0)[0]\n",
    "\n",
    "median_age = calculate_median('Age')  # Median age\n",
    "median_fare = calculate_median('Fare')  # Median fare\n",
    "\n",
    "titanic = titanic.fillna({\n",
    "    'Pclass': -1,\n",
    "    'Sex': 'Unknown',\n",
    "    'Age': median_age,\n",
    "    'SibSp': -1,\n",
    "    'Parch': -1,\n",
    "    'Fare': median_fare,\n",
    "    'Embarked': 'Unknown'\n",
    "})\n",
    "\n",
    "# Feature Engineering\n",
    "titanic = titanic.withColumn('FamilySize', col('SibSp') + col('Parch') + 1)\n",
    "titanic = titanic.withColumn('IsAlone', when(col('FamilySize') == 1, 1).otherwise(0))\n",
    "\n",
    "# Define categorical features \n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'IsAlone']\n",
    "\n",
    "# Define numerical features \n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "stages = []\n",
    "for col_name in categorical_features:\n",
    "    string_indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{col_name}_Index\"], outputCols=[f\"{col_name}_Vec\"])\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "# Scaling numerical features \n",
    "for col_name in numerical_features:\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=f\"vec_{col_name}\")\n",
    "    scaler = StandardScaler(inputCol=f\"vec_{col_name}\", outputCol=f\"scaled_{col_name}\", withStd=True, withMean=False)\n",
    "    stages += [assembler, scaler]\n",
    "\n",
    "# Create a pipeline and transform the data\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(titanic)\n",
    "titanic = pipeline_model.transform(titanic)\n",
    "\n",
    "# Drop intermediate columns created during scaling and one-hot encoding\n",
    "titanic = titanic.drop('vec_Age', 'vec_Fare', 'vec_FamilySize', 'vec_SibSp', 'vec_Parch', 'Pclass_Index', 'Sex_Index', 'Embarked_Index', 'IsAlone_Index')\n",
    "\n",
    "# Drop original categorical columns (no longer needed after one-hot encoding)\n",
    "titanic = titanic.drop(*categorical_features)\n",
    "\n",
    "# Drop original numeric columns (no longer needed after scaling)\n",
    "titanic = titanic.drop(*numerical_features)\n",
    "\n",
    "vector_columns = [\"Pclass_Vec\", \"Sex_Vec\", \"Embarked_Vec\", \"IsAlone_Vec\", \"scaled_Age\", \"scaled_Fare\", \"scaled_FamilySize\", \"scaled_SibSp\", \"scaled_Parch\"]\n",
    "\n",
    "def to_dense(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "to_dense_udf = udf(to_dense, ArrayType(FloatType()))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    titanic = titanic.withColumn(vector_col, to_dense_udf(col(vector_col)))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    num_features = len(titanic.select(vector_col).first()[0])  # Getting the size of the vector\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        titanic = titanic.withColumn(f\"{vector_col}_{i}\", col(vector_col).getItem(i))\n",
    "    \n",
    "    titanic = titanic.drop(vector_col)\n",
    "\n",
    "# Save the processed data to GCS\n",
    "titanic.write.parquet(args.processed_data_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source code for PySpark\n",
    "\n",
    "We need to upload our PySpark code to our GCS bucket to be referenced by the `DataprocPySparkBatchOp` component in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $PYSPARK_DIR/preprocessing.py $PREPROCESSING_PYTHON_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom training job\n",
    "In this section, we will create our custom training job. It will consist of the following steps:\n",
    "1. Create a Google Artifact Registry repository to host our custom container image.\n",
    "2. Create our custom training script.\n",
    "3. Create a Dockerfile that will specify how to build our custom container image. \n",
    "4. Build our custom container image.\n",
    "5. Push our custom container image to Google Artifact Registry so that we can use it in subsequent steps in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Google Artifact Registry repository\n",
    "\n",
    "Our custom training component in our pipeline will run in a container on the Vertex AI Training service. In this section, we will create the Google Artifact Registry repository in which we can store our custom container image that we will build in later steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $TRAIN_REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"Train repo for MLOps workload\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "399eba3ab133"
   },
   "source": [
    "## Define the code for our training job\n",
    "\n",
    "The following code will create a file that contains the code for our custom training job. \n",
    "\n",
    "The code performs the following processing steps:\n",
    "\n",
    "1. Imports required libraries and sets initial variable values based on arguments passed to the script (the arguments are described below).\n",
    "2. Reads in the processed dataset that was created by the data preprocessing step in our pipeline.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train_model(args):\n",
    "    # Input arguments\n",
    "    project_id = args.project_id\n",
    "    bucket_name = args.bucket_name\n",
    "    processed_data_path = args.processed_data_path\n",
    "    model_path = args.model_path\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    n_hidden_layers = args.n_hidden_layers\n",
    "    n_units = args.n_units\n",
    "    activation_fn = args.activation_fn\n",
    "    test_data_file_name = args.test_data_file_name\n",
    "    \n",
    "    ### DATA PREPARATION SECTION ###\n",
    "    \n",
    "    # Get list of all Parquet files created by our preprocessing step in our GCS directory\n",
    "    fs = gcsfs.GCSFileSystem(project=project_id)  # replace with your project name\n",
    "    files = [f for f in fs.ls(processed_data_path) if 'part' in os.path.basename(f)]\n",
    "\n",
    "    print(f\"Found files: {files}\")\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No Parquet files found in directory: {processed_data_path}\")\n",
    "\n",
    "    # Read all Parquet files and concatenate into a single DataFrame\n",
    "    dfs = [pd.read_parquet('gs://' + file) for file in files]\n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Separate the target and input features in the dataset\n",
    "    y = data['Survived'].values.astype('float32') # Ensuring the target column has a consistent data type\n",
    "    X = data.drop('Survived', axis=1)\n",
    "\n",
    "    # Convert X to NumPy array (required input for training)\n",
    "    X = X.values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    ### MODEL TRAINING AND EVALUATION SECTION ###\n",
    "\n",
    "    # Define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation=activation_fn, input_shape=(X_train.shape[1],)))\n",
    "    for _ in range(n_hidden_layers - 1):\n",
    "        model.add(Dense(n_units, activation=activation_fn))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test)\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Get the model predictions\n",
    "    y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "    # Compute the AUC\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}, AUC: {auc}')\n",
    "\n",
    "    # Save the model to GCS\n",
    "    model.save(model_path)\n",
    "    \n",
    "    ### SAVING TEST DATA FOR LATER REFERENCE ###    \n",
    "    # Converting the test dataset to JSON Lines format and saving it to GCS\n",
    "    \n",
    "    # Convert numpy array to list of lists\n",
    "    X_test_list = X_test.tolist()\n",
    "    \n",
    "    # Create a JSONL string from the list of lists\n",
    "    jsonl_str = \"\\n\".join(json.dumps(instance) for instance in X_test_list)\n",
    "    \n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Get the bucket details\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Create the blob (See https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.blob.Blob)\n",
    "    blob = bucket.blob(test_data_file_name)\n",
    "    \n",
    "    # Upload the JSONL string to the blob\n",
    "    blob.upload_from_string(jsonl_str)\n",
    "    \n",
    "    # Return the trained model\n",
    "    return model  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network model for Titanic survival prediction')\n",
    "    \n",
    "    parser.add_argument('--project_id', type=str, help='GCP Project ID')\n",
    "    parser.add_argument('--bucket_name', type=str, help='GCP Bucket ID')\n",
    "    parser.add_argument('--processed_data_path', type=str, help='Path to the directory containing the preprocessed data')\n",
    "    parser.add_argument('--test_data_file_name', type=str, help='Path to the directory containing the preprocessed data')\n",
    "    parser.add_argument('--model_path', type=str, help='Path to save the trained model')\n",
    "    parser.add_argument('--n_hidden_layers', type=int, default=2, help='Number of hidden layers')\n",
    "    parser.add_argument('--n_units', type=int, default=64, help='Number of units per layer')\n",
    "    parser.add_argument('--activation_fn', type=str, default='relu', help='Activation function for hidden layers')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='Number of epochs')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "The requirements.txt file is a convenient way to specify all of the packages that we want to install in our custom container image. This file will be referenced in the Dockerfile for our image.\n",
    "\n",
    "In this case, we will install:\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)\n",
    "* [Filesystem interfaces for Python](https://filesystem-spec.readthedocs.io/en/latest/)\n",
    "* [GCSFS](https://gcsfs.readthedocs.io/en/latest/)\n",
    "* [pyarrow](https://arrow.apache.org/docs/python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/requirements.txt\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage\n",
    "fsspec==2023.5.0\n",
    "gcsfs==2023.5.0\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the Docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our custom training image\n",
    "\n",
    "The steps required to build our image are:\n",
    "\n",
    "1. Change directory to our application directory.\n",
    "2. Build Docker image.\n",
    "3. Push the image to our Google Artifact Registry.\n",
    "4. Change directory back to our parent application directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $APPLICATION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build ./ -t $TRAIN_IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push our custom image to Google Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQd9M1_9bif7"
   },
   "source": [
    "# Define our Vertex AI Pipeline\n",
    "\n",
    "Now that we have defined our custom data preprocessing and model training components, it's time to define our MLOps pipeline.\n",
    "\n",
    "In this section, we will use the Kubeflow Pipelines SDK and Google Cloud Pipeline Components to define our MLOps pipeline.\n",
    "\n",
    "We begin by specifying all of the required variables in our pipeline, and populating their values from the constants we defined earlier in our notebook. We then specify the following components in our pipeline:\n",
    "\n",
    "1. [DataprocPySparkBatchOp](https://cloud.google.com/vertex-ai/docs/pipelines/dataproc-component) to perform our data preprocessing step.\n",
    "2. [CustomTrainingJobOp](https://cloud.google.com/vertex-ai/docs/pipelines/customjob-component#customjobop) to perform our custom model training step.\n",
    "3. [importer](https://www.kubeflow.org/docs/components/pipelines/v2/components/importer-component/) to import our [UnmanagedContainerModel](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.UnmanagedContainerModel) object.\n",
    "4. [ModelUploadOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/model.html#v1.model.ModelUploadOp) to upload our Model artifact into Vertex AI Model Registry.\n",
    "5. [EndpointCreateOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.EndpointCreateOp) to create a Vertex AI [Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints).\n",
    "6. [ModelDeployOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.ModelDeployOp) to deploy our Google Cloud Vertex AI Model to an Endpoint, creating a [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) object within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX2u9hXDWtpp"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME, description=\"MLOps pipeline for custom data preprocessing, model training, and deployment.\")\n",
    "def pipeline(\n",
    "    bucket_name: str = BUCKET,\n",
    "    display_name: str = PIPELINE_NAME,\n",
    "    preprocessing_main_python_file_uri: str = PREPROCESSING_PYTHON_FILE_URI,\n",
    "    preprocessing_args: list = PREPROCESSING_ARGS,\n",
    "    processed_data_path: str = PROCESSED_DATA_URI,\n",
    "    model_path: str = MODEL_URI,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    subnetwork_uri: str = SUBNETWORK_URI,\n",
    "    dataproc_runtime_version: str = DATAPROC_RUNTIME_VERSION,\n",
    "    worker_pool_specs: list = WORKER_POOL_SPEC,\n",
    "    base_output_directory: str = PIPELINE_ROOT,\n",
    "    explanation_parameters: dict = EXPLANATION_PARAMS,\n",
    "    serving_image_uri: str = SERVING_IMAGE_URI,\n",
    "    endpoint_name: str = ENDPOINT_NAME\n",
    "):\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessing_op = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        main_python_file_uri=preprocessing_main_python_file_uri,\n",
    "        args=preprocessing_args,\n",
    "        subnetwork_uri=subnetwork_uri,\n",
    "        runtime_config_version=dataproc_runtime_version,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model_training_op = custom_job.CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        display_name=\"train-mlops-model\",\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "    ).after(preprocessing_op)\n",
    "    \n",
    "    importer_op = dsl.importer(\n",
    "        artifact_uri=model_path,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": serving_image_uri,\n",
    "            },\n",
    "        },\n",
    "    ).after(model_training_op)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=model_name,\n",
    "        unmanaged_container_model=importer_op.outputs[\"artifact\"],\n",
    "        explanation_parameters=explanation_parameters,\n",
    "    ).after(importer_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=endpoint_name,\n",
    "    ).after(model_upload_op)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    ).after(endpoint_create_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ePytY8t3bu"
   },
   "source": [
    "### Compile our pipeline into a YAML file\n",
    "\n",
    "Now that we have defined out pipeline structure, we need to compile it into YAML format in order to run it in Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFyCaNPIWtsU"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, 'mlops-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq26zYhQb0qm"
   },
   "source": [
    "## Submit and run our pipeline in Vertex AI Pipelines\n",
    "\n",
    "Now we're ready to use the Vertex AI Python SDK to submit and run our pipeline in Vertex AI Pipelines.\n",
    "\n",
    "The parameters, artifacts, and metrics produced from the pipeline run are automatically captured into Vertex AI Experiments as an experiment run. We will discuss the concept of Vertex AI Experiments in more detail in laer chapters in the book. The output of the following cell will provide a link at which you can watch your pipeline as it progresses through each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwzYIoEabwRx"
   },
   "outputs": [],
   "source": [
    "pipeline = aiplatform.PipelineJob(display_name=PIPELINE_NAME, template_path='mlops-pipeline.yaml', enable_caching=False)\n",
    "\n",
    "pipeline.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e428aab1826"
   },
   "source": [
    "### Wait for the pipeline to complete\n",
    "The following function will periodically print the status of our pipeline execution. If all goes to plan, you will eventually see a message saying \"PipelineJob run completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b05ed7a9cf3d"
   },
   "outputs": [],
   "source": [
    "pipeline.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!! You have officially created and implemented an MLOps pipeline on Vertex AI!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's send an inference request to our new model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been deployed to a Vertex AI Endpoint, we can start sending inference requests to it! In the real world, inference requests may come from a variety of potential sources. In our case, our training script created a small subset of our processed dataset to use for testing purposes. For convenience and example purposes, our training script also saved that test dataset to GCS. We can use it now to send inference requests to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the test dataset to a local directory in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $TEST_DATASET_PATH $TEST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model endpoint details\n",
    "\n",
    "In order to test our model, we first need to get the details of our newly-deployed model endpoint in Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlops_endpoint_list = aiplatform.Endpoint.list(filter=f'display_name={ENDPOINT_NAME}', order_by='create_time desc')\n",
    "new_mlops_endpoint = mlops_endpoint_list[0]\n",
    "endpoint_resource_name = new_mlops_endpoint.resource_name\n",
    "print(endpoint_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send inference requests to our model\n",
    "Let's go ahead and test it out! The following code will read a record from our test dataset and send it in an inference request to our model endpoint in Vertex AI.\n",
    "Our model should provide a prediction response that will be printed below the code cell. It should be a number between 0 and 1, which predicts the probability of survival for that record. Numbers closer to zero predict a low probability of survival, while numbers closer to 1 predict a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = LOCAL_TEST_DATASET_PATH\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    # Read the first line of the file\n",
    "    line = f.readline()\n",
    "\n",
    "    # Convert JSON line to Python dictionary\n",
    "    instance = json.loads(line)\n",
    "    \n",
    "    # Convert to a list of lists (required for our model input)\n",
    "    instance_list = [instance]\n",
    "\n",
    "    # Send the inference request\n",
    "    response = aiplatform.Endpoint(endpoint_resource_name).predict(instance_list)\n",
    "\n",
    "    # Print the response\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "# WARNING: THE MODEL AND ENDPOINT CREATED IN THIS NOTEBOOK ARE ALSO USED IN CHAPTER 12. IF YOU PLAN TO PROCEED WITH THE ACTIVITIES IN CHAPTER 12, DO NOT DELETE THESE RESOURCES YET.\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete Vertex AI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import exceptions as gcp_exceptions\n",
    "if clean_up:  \n",
    "    try:\n",
    "        endpoint_list = aiplatform.Endpoint.list(filter=f'display_name=\"{ENDPOINT_NAME}\"')\n",
    "        if endpoint_list:\n",
    "            endpoint = endpoint_list[0]  # Assuming only one endpoint with that name\n",
    "\n",
    "            # Undeploy all models (if any)\n",
    "            try:\n",
    "                endpoint.undeploy_all()\n",
    "                print(f\"Undeployed all models from endpoint: {ENDPOINT_NAME}\")\n",
    "            except gcp_exceptions.NotFound:\n",
    "                print(f\"No models found to undeploy from endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:  # Catching general errors for better debugging\n",
    "                print(f\"Unexpected error while undeploying models: {e}\")\n",
    "\n",
    "            # Delete endpoint\n",
    "            try:\n",
    "                endpoint.delete()\n",
    "                print(f\"Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "        else:\n",
    "            print(f\"No endpoint found matching: {ENDPOINT_NAME}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Endpoint not found: {ENDPOINT_NAME}\")\n",
    "\n",
    "    # Delete models\n",
    "    try:\n",
    "        model_list = aiplatform.Model.list(filter=f'display_name=\"{MODEL_NAME}\"')\n",
    "        if model_list:\n",
    "            for model in model_list:\n",
    "                print(f\"Deleting model: {model.display_name}\")\n",
    "                model.delete()\n",
    "        else:\n",
    "            print(f\"No models found matching: {MODEL_NAME}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Model not found: {MODEL_NAME}\")\n",
    "\n",
    "    # Delete pipeline\n",
    "    try:\n",
    "        pipeline.delete()\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(\"error deleting pipeline\")\n",
    "\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete artifact repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4a76167aee4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    try:\n",
    "        # Delete the artifact repository\n",
    "        ! gcloud artifacts repositories delete $TRAIN_REPO_NAME --location=$REGION --quiet\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting artifact registry: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_mlops.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
