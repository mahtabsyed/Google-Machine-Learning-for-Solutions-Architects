{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Apache Beam on Google Cloud Dataflow to process streaming data\n",
    "\n",
    "This notebook example demonstrates how to set up a streaming pipeline that processes a stream that contains NYC taxi ride data.\n",
    "\n",
    "It's a modified version of an example from Google's example repository, and it reads the data from a public Google Cloud Pub/Sub topic that Google created for people to be able to test streaming data processing use-cases.\n",
    "\n",
    "Each element in the stream contains the location of the taxi, the timestamp, the meter reading, the meter increment, the passenger count, and ride status in JSON format.\n",
    "\n",
    "We will use Apache Beam to define the pipeline, run it on Google Cloud Dataflow, and store the results in Google Cloud BigQuery.\n",
    "To get a better understanding of the Apache Beam constructs we use in this example, see [Apache Beam Basics](https://beam.apache.org/documentation/basics/), and the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
    "* [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n",
    "* [Dataflow Pricing](https://cloud.google.com/dataflow/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "import google.auth\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set additional variables, such as the relevant BigQuery dataset and table, the Google Cloud Pub/Sub topic that our pipeline will read from, and the Google Cloud Storage bucket that Dataflow will use to store the data in transit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Storage location for Dataflow artifacts.\n",
    "DATAFLOW_GCS_PATH = f'gs://{BUCKET}/dataflow'\n",
    "\n",
    "# BigQuery dataset ID and table\n",
    "DATASET_ID = \"taxirides\"\n",
    "TABLE_ID = \"run_rates\"\n",
    "\n",
    "# Pub/Sub source topic\n",
    "topic = \"projects/pubsub-public-data/topics/taxirides-realtime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the options to create the streaming pipeline.\n",
    "For more information, see [Apache Beam Pipeline Options](https://beam.apache.org/releases/pydoc/2.4.0/apache_beam.options.pipeline_options.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Beam pipeline options.\n",
    "options = beam.options.pipeline_options.PipelineOptions(flags={})\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(beam.options.pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "# The project will be used for creating a subscription to the PubSub topic.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline with the options we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a `PTransform` that will create a subscription to the given Pub/Sub topic and reads from the subscription. \n",
    "The data is in JSON format, so we add another `Map` `PTransform` to parse the data as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic) | beam.Map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are reading from an unbounded source, we need to create a windowing scheme.\n",
    "We will use sliding windows with a 10-second duration for each window, and with one second for each slide.\n",
    "For more information about windowing in Apache Beam, see [Windowing Basics](https://beam.apache.org/documentation/programming-guide/#windowing-basics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_data = (data | \"window\" >> beam.WindowInto(beam.window.SlidingWindows(10, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there will be some duplicate data for each element because with sliding windows each element has to appear in multiple\n",
    "windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the 10-second dollar run rate for each second, by summing the `meter_increment` JSON field for each window.\n",
    "\n",
    "First, extract the `meter_increment` field from the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_increments = windowed_data | beam.Map(lambda e: e.get('meter_increment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all elements by window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rates = meter_increments | beam.CombineGlobally(sum).without_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our pipeline defined. Next, we will specify some additional options in order to run this pipeline on Google Cloud Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Google Cloud region to run Dataflow.\n",
    "options.view_as(GoogleCloudOptions).region = REGION\n",
    "\n",
    "# Set the staging location. This location is used to stage the\n",
    "# Dataflow pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = f'{DATAFLOW_GCS_PATH}/staging'\n",
    "\n",
    "# Set the temporary location. This location is used to store temporary files\n",
    "# or intermediate results before outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = f'{DATAFLOW_GCS_PATH}/temp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the schema for our BigQuery Table. Dataflow will create the table in BigQuery, with this schema. The outputs are just the aggregated run_rates, so our schema consists of just one field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = {\n",
    "    'fields': [{\n",
    "        'name': 'run_rates', 'type': 'NUMERIC', 'mode': 'NULLABLE'\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the process is to write the results (i.e., the contents of the run_rates variable we created above) to BigQuery. However, the run_rates variable currently just contains numeric data, whereas the WriteToBigQuery transform expects a dictionary with a key-value pair that matches the schema. We also want to ensure that we don't exceed the size of BigQuery's NUMERIC datatype, so we round the rate value to 5 decimal places just to be safe. The following code will perform the necessary conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert run_rates to a dictionary\n",
    "def to_dict(rate):\n",
    "    return {'run_rates': round(rate, 5)}\n",
    "\n",
    "run_rates_dict = run_rates | 'ConvertToDict' >> beam.Map(to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready to be written to BigQuery, and the following code will create the BigQuery sink for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to BigQuery\n",
    "run_rates_dict | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(\n",
    "    table=f'{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}',\n",
    "    schema=table_schema,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have defined all of the steps in our data processing pipeline, and now we will run the pipeline on Google Cloud Dataflow. After running this cell, it will display details regarding the pipeline result, and you can go to the [Dataflow Jobs Console](https://console.cloud.google.com/dataflow/jobs/) to view the additional job details and execution status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runner = beam.runners.DataflowRunner()\n",
    "result = runner.run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_id = result.job_id()\n",
    "print(f\"Job ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the Dataflow Job after verifying\n",
    "\n",
    "After verifying the data processing as described in the chapter, you can stop the Dataflow job as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud dataflow jobs cancel $job_id --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the job status either in the [Dataflow Jobs Console](https://console.cloud.google.com/dataflow/jobs/) or with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud dataflow jobs list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When you are finished this chapter you should repeat the same `gcloud dataflow jobs cancel` command for each running job (replace the `job_id` each time). **Otherwise, you will continue to be charged for those jobs.** \n",
    "\n",
    "**It's also possible to stop and delete the jobs from the [Dataflow Jobs Console](https://console.cloud.google.com/dataflow/jobs/).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete BigQuery datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we created a BigQuery dataset outside this notebook, so it's best to delete it from the [BigQuery console](https://console.cloud.google.com/bigquery). In the console, click on the name of your project to see the datasets in that project. Then, click the three vertical dots next to the name of the dataset, and click `Delete`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket. \n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "apache-beam-2.56.0",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Apache Beam 2.56.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.56.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
