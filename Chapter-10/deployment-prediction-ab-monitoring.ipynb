{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffcd19a",
   "metadata": {},
   "source": [
    "# Deploying a model to Vertex AI and getting predictions from the model\n",
    "\n",
    "In this notebook, we will train and deploy an ML model in Google Cloud, and get predictions from our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
    "* [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef1185",
   "metadata": {},
   "source": [
    "First install the latest version of the Vertex AI library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42518795-ffa4-4c3f-a5c5-0e76438c0b34",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84977a8f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google.cloud.aiplatform --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96f677-eccf-42cb-a2b9-48be925511ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall protobuf -y\n",
    "!pip install protobuf==3.19.* --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5092e06-64ed-485f-9e24-e1198271b83f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefcdf0-4ef3-4895-8700-347f8cdeceef",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed6aab-ab90-4f69-b6d7-3293174cc9a4",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ec60a-9dcf-415d-bcfb-566dc58e911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c534-09a1-49e1-a6a5-3fd1b0417081",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6de65-79e8-46ba-a1e8-2959d22f96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868271aa-3ec1-45d7-be9d-c36aef4ef395",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2de5e-c2ef-40a1-a49e-9614f1107b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1592827-4f0c-442c-beb7-04e18f2bb58e",
   "metadata": {},
   "source": [
    "## Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21650ab2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other re-usable variables (no need to change these)\n",
    "TEST_DATA_FILENAME=\"housing_test_data.jsonl\"\n",
    "MODEL_NAME=\"housing_model\"\n",
    "TEST_DATA_LOCATION=f\"gs://{BUCKET}/data/deployment-chapter\"\n",
    "MODEL_LOCATION=f\"gs://{BUCKET}/models/deployment-chapter/tensorflow\"\n",
    "OUTPUT_LOCATION=f\"gs://{BUCKET}/outputs/deployment-chapter\"\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(housing.data)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, housing.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5633c",
   "metadata": {},
   "source": [
    "## Create and train the TensorFlow model\n",
    "\n",
    "Next, we actually training a model using TensorFlow.\n",
    "Again, you can ignore the warnings if any are shown when you execute the code. You will also see results from each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72391c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "tf_model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(0.01))\n",
    "history = tf_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8f2d9",
   "metadata": {},
   "source": [
    "## Save the trained model\n",
    "Next, we save the trained model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2996bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506714a",
   "metadata": {},
   "source": [
    "Note the model input names to be used when creating the batch prediction test data later in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf49a4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_input_names = tf_model.input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a2f84",
   "metadata": {},
   "source": [
    "## Copy the model to GCS\n",
    "\n",
    "We copy our model to GCS to that we can use it with Vertex AI prediction service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c486dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $MODEL_NAME $MODEL_LOCATION/$MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f77109",
   "metadata": {},
   "source": [
    "## Create a Google Cloud Vertex AI Model object\n",
    "\n",
    "The process of creating a Google Cloud Vertex AI Model object from our trained model involves several steps:\n",
    "\n",
    "1. Serialize the model into a format that Vertex AI can understand.\n",
    "2. Upload the serialized model to our Google Cloud Storage bucket.\n",
    "3. Create a Vertex AI Model resource, pointing it to the serialized model in the Cloud Storage bucket. This step will add the model to the Vertex AI Model Registry.\n",
    "\n",
    "The code to do that is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c1b68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Create a Vertex AI Model resource\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=f\"{MODEL_LOCATION}/{MODEL_NAME}\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ecd29",
   "metadata": {},
   "source": [
    "## Convert the test dataset to JSON Lines to be used with our model\n",
    "\n",
    "Our model expects the test data to be in JSON Lines format. The following code converts it accordingly, and writes it to a file that we can then upload to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94606993",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert X_test to a DataFrame\n",
    "df = pd.DataFrame(X_test, columns=housing.feature_names)\n",
    "\n",
    "# Convert the DataFrame to a list of dict records\n",
    "records = df.to_dict('records')\n",
    "\n",
    "# Write out the records in JSON Lines format\n",
    "with open(f'{TEST_DATA_FILENAME}', 'w') as f:\n",
    "    for record in records:\n",
    "        json.dump({model_input_names[0]: list(record.values())}, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa121e5c",
   "metadata": {},
   "source": [
    "## Upload the file to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20043ccc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp $TEST_DATA_FILENAME $TEST_DATA_LOCATION/$TEST_DATA_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1babae",
   "metadata": {},
   "source": [
    "## Create a batch prediction job to get predictions from our model\n",
    "\n",
    "Next, we create the actual batch prediction job on Vertex AI. The job may take 20 minutes or more to complete. This is because Vertex AI spins up the infrastructure to run our batch job, such as the servers and containers, as well as networking infrastructure, then loads our model and input data, and executes our batch prediction job. The execution time also depends on the size of your model and input data. For example, a production job that processes large amounts of data may run for a much longer time.\n",
    "\n",
    "The status of the job will be displayed periodically below this code cell. If all goes well, you will eventually see a status message saying \"JobState.JOB_STATE_SUCCEEDED\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc47e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.BatchPredictionJob.create(\n",
    "    job_display_name=\"housing_prediction\",\n",
    "    model_name=model.resource_name,\n",
    "    gcs_source=f\"{TEST_DATA_LOCATION}/{TEST_DATA_FILENAME}\",\n",
    "    gcs_destination_prefix=OUTPUT_LOCATION,\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00a4ac",
   "metadata": {},
   "source": [
    "## Get the location of the prediction results in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71f049",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JOB_OUTPUT_DIRECTORY_PATH=job.output_info.gcs_output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35509d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_directory_name=JOB_OUTPUT_DIRECTORY_PATH.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885adca",
   "metadata": {},
   "source": [
    "## Copy the predictions from GCS into our notebook for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf38513",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -r $JOB_OUTPUT_DIRECTORY_PATH ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe1164",
   "metadata": {},
   "source": [
    "## Load and print out the predictions\n",
    "\n",
    "We'll just print the first 10 lines to view a subset of the prediction outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467ccfb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_count = 0 \n",
    "\n",
    "with open(f\"{output_directory_name}/prediction.results-00000-of-00001\") as f:\n",
    "    for line in f:\n",
    "        if line_count < 10:\n",
    "            print(json.loads(line))\n",
    "            line_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c7154",
   "metadata": {},
   "source": [
    "In each line, the 'prediction' is our model's predicted price for that house, given the input features in that line.\n",
    "\n",
    "Now that we have our model defined, we can run a batch inference job whenever we wish, such as immediately when new data becomes available, or every night (with new data from the prior day)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682ca0f",
   "metadata": {},
   "source": [
    "# Online Inference\n",
    "\n",
    "Next, let's deploy our model for online inference. I recommend referencing chapter 1 of the book at this point, in which I discuss what would be required to host your models on your own infrastructure. It would require a lot of work and financial investment!\n",
    "\n",
    "**In vertex AI, however, it's only going to take one simple line of code!**\n",
    "\n",
    "This is astonishingly simple, considering all of the work it's going to do on our behalf in order to put our model into production.\n",
    "\n",
    "Also notice how easy it is to enable autoscaling for our model. All we have to do is specify the minimum and maximum number of nodes we want to configure, and Vertex AI will take care of the rest. We do this by configuring min_replica_count and max_replica_count.\n",
    "\n",
    "The following piece of code will reference our model in the Vertex AI Model Registry, and deploy it to a Vertex AI Prediction endpoint for online inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b704bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\", min_replica_count=1, max_replica_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ab697",
   "metadata": {},
   "source": [
    "## Get our endpoint ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d954a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name\n",
    "print(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da30da-7374-43d3-9c8a-d72905d82a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece40235-9d20-4c86-9fd6-bf622488dae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = endpoint.display_name\n",
    "print(ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6ee48",
   "metadata": {},
   "source": [
    "## Get our deployed model ID\n",
    "\n",
    "We can get a list of models deployed to our endpoint. At this point, we only have one model deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84f16c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployed_models = endpoint.list_models()\n",
    "deployed_model_id = deployed_models[0].id\n",
    "print(deployed_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55428794",
   "metadata": {},
   "source": [
    "## Create and execute an online inference request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337405c",
   "metadata": {},
   "source": [
    "### Specify the data to be used in our inference request\n",
    "\n",
    "Earlier in this notebook, we already split our housing dataset into multiple subsets as part of the training process.\n",
    "One subset is X_test, which represents the portion of our dataset that was not used to train our model (i.e., it was reserved for testing purposes), and which only contains the housing features (i.e., it does not contain the target label column, \"price\").\n",
    "\n",
    "We will use elements (or observations) from that dataset to test our model.\n",
    "\n",
    "First, let's test with just a single observation from the dataset, which represents a single house in our dataset. To make things simple, we will use the first house in the dataset.\n",
    "\n",
    "Note that X_test is a NumPy array, but our model expects to receive a list of float numbers as input, so we will convert the input to a list of float numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebe263",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_instance = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76236fdb",
   "metadata": {},
   "source": [
    "### Execute the prediction request\n",
    "\n",
    "The next piece of code sends a prediction request to our endpoint, using the input we defined in the previous piece of code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7f32f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint.predict([test_instance])\n",
    "\n",
    "# Print out the prediction\n",
    "print(\"Prediction result:\", response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70c05c",
   "metadata": {},
   "source": [
    "There you have it! We've successfully send an inference request to a model that is hosted on an endpoint in Vertex AI. That inference request contained details of a specific house in our dataset, and our model returned it's predicted price for that house, based on the inputs provided in our request!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326413b7",
   "metadata": {},
   "source": [
    "We could also repeat this process to get multiple predictions in a single request, if we'd like.\n",
    "\n",
    "In this case, we take the first three instances from our X_test dataset, convert each one a list of float numbers, and send those details in our request.\n",
    "Our model then returns a list containing the predictions for each of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d06e63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_instances = [x.tolist() for x in X_test[:3]]\n",
    "\n",
    "# Make the prediction request\n",
    "response = endpoint.predict(test_instances)\n",
    "\n",
    "# Print out the prediction\n",
    "print(\"Prediction result:\", response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52544c87",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "Next, let's test a new version of our model and compare it against our current model using A/B testing.\n",
    "\n",
    "We'll repeat previous steps in this notebook, and create a new model that was trained using 100 epochs instead of 20.\n",
    "\n",
    "Then we'll see how that model performs in comparison to our prior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b1c8f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_100_NAME=\"housing_model_100\"\n",
    "\n",
    "model_100 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_100.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(0.01))\n",
    "history = model_100.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "model_100.save(MODEL_100_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d2159",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $MODEL_100_NAME $MODEL_LOCATION/$MODEL_100_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af380f23",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Vertex AI Model resource\n",
    "model_100 = aiplatform.Model.upload(\n",
    "    display_name=MODEL_100_NAME,\n",
    "    artifact_uri=f\"{MODEL_LOCATION}/{MODEL_100_NAME}\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23946b9d",
   "metadata": {},
   "source": [
    "## Note current traffic split settings\n",
    "\n",
    "In order to understand how the traffic_split settings work, let's check the current configuration before we deploy another model to our endpoint.\n",
    "We currently only have one model deployed, so that model should be getting 100% of the traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c476c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778aacb",
   "metadata": {},
   "source": [
    "## Deploy our new model to our endpoint\n",
    "\n",
    "We will deploy our new model to the same endpoint that we aleady deployed. We do this with the line that says *endpoint=endpoint*.\n",
    "If we omitted that line then Vertex AI would create a new dedicated endpoint for our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ea6f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_100.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=MODEL_100_NAME,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1, \n",
    "        max_replica_count=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11145e99",
   "metadata": {},
   "source": [
    "## Check the traffic split settings\n",
    "\n",
    "If we don’t set any value for the traffic_split variable, the default behavior is to keep all traffic directed to the original model that was already deployed to our endpoint. This is a safety mechanism that prevents unexpected behavior in terms of how our models serve traffic from our clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea17cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24195c",
   "metadata": {},
   "source": [
    "We can see that we now have two deployed models, but the newly deployed model is not yet receiving any traffic.\n",
    "Let's take a note of both deployed model IDs, so we can use them to update the traffic_split configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be953e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployed_models = endpoint.list_models()\n",
    "model_ids = []\n",
    "\n",
    "for deployed_model in deployed_models:\n",
    "    model_ids.append(deployed_model.id)\n",
    "\n",
    "model_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b076a",
   "metadata": {},
   "source": [
    "## Update the traffic split settings\n",
    "A common approach would be to test our new model with a small portion of our traffic; perhaps 10%.\n",
    "\n",
    "Let's update our traffic split settings to allocate 10% of traffic to our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf2645",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "traffic_split = {model_ids[0]: 90, model_ids[1]: 10}\n",
    "\n",
    "endpoint.update(traffic_split=traffic_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac310a9",
   "metadata": {},
   "source": [
    "## Enable prediction request-response logging\n",
    "\n",
    "The prediction request-response logging feature will log our models’ responses for the prediction requests received. We can save those responses in a Google Cloud BigQuery table, which enables us to perform analysis on the prediction responses from each of our models, and see how they are performing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff847",
   "metadata": {},
   "source": [
    "### Create BigQuery table for the logs\n",
    "\n",
    "The next two cells are technically optional but it can take some time for the BigQuery table to be created, so it's better for our purposes here to specify the BigQuery table details and create it explicitly in this manner.\n",
    "Without this part, a dataset and table would be automatically created after some time.\n",
    "\n",
    "The BigQuery table needs to be created in this specific way, due to how the prediction request-response logging feature currently works (it has strict and rigid requirements for how the table and schema need to be created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b8db0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(PROJECT_ID)\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Specify the dataset_id within the project\n",
    "dataset_id = f'cpt10_{ENDPOINT_ID}'\n",
    "\n",
    "# Create a DatasetReference using a chosen dataset ID\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "# Construct a full Dataset object to send to the API\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "\n",
    "# Specify the geographic location where the dataset should reside\n",
    "dataset.location = REGION\n",
    "\n",
    "# Specify the table_id within the dataset\n",
    "table_id = 'request_response_logging'\n",
    "\n",
    "log_table_ref_id = f'{PROJECT_ID}.{dataset_id}.{table_id}'\n",
    "\n",
    "# Create the new dataset\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset)  # Make an API request\n",
    "    print(f\"Created dataset {client.project}.{dataset.dataset_id}\")\n",
    "except google.api_core.exceptions.Conflict:\n",
    "    print(f\"Dataset {client.project}.{dataset.dataset_id} already exists\")\n",
    "\n",
    "# Set your query\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE `{log_table_ref_id}` (\n",
    "  endpoint STRING,\n",
    "  deployed_model_id STRING,\n",
    "  logging_time TIMESTAMP,\n",
    "  request_id NUMERIC\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(create_table_query)  \n",
    "\n",
    "# Wait for the job to complete\n",
    "query_job.result() \n",
    "\n",
    "print(\"Table created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55605bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = client.get_table(log_table_ref_id) \n",
    "\n",
    "original_schema = table.schema\n",
    "new_schema = original_schema[:]  # Creates a copy of the schema.\n",
    "new_schema.append(bigquery.SchemaField(\"request_payload\", \"STRING\", mode=\"REPEATED\"))\n",
    "new_schema.append(bigquery.SchemaField(\"response_payload\", \"STRING\", mode=\"REPEATED\"))\n",
    "\n",
    "table.schema = new_schema\n",
    "table = client.update_table(table, [\"schema\"])\n",
    "\n",
    "if len(table.schema) == len(new_schema):\n",
    "    print(\"A new schema has been added.\")\n",
    "else:\n",
    "    print(\"The schema has not been modified.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37380eec",
   "metadata": {},
   "source": [
    "### Update the request-response logging configuration\n",
    "\n",
    "At the time of writing this in August 2023, the only supported way to enable prediction request-response logging is by using the Vertex AI REST API. Check the documentatiopn [here](https://cloud.google.com/vertex-ai/docs/predictions/online-prediction-logging#enable_disable_logs-drest) for details.\n",
    "\n",
    "The following code will generate an access token that we can use to make a request to the Vertex AI REST API.\n",
    "It will then use the Python *requests* library to build and send an HTTPS request to the Vertex AI REST API.\n",
    "The body of the request sets the predict_request_response_logging_config to enable the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426de791",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "\n",
    "OUTPUT_URI = f\"bq://{log_table_ref_id}\"\n",
    "\n",
    "# Execute gcloud command to get access token\n",
    "get_token_command = \"gcloud auth print-access-token\"\n",
    "TOKEN = subprocess.getoutput(get_token_command)\n",
    "\n",
    "# URL\n",
    "url = f\"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Body\n",
    "body = {\n",
    "    \"predict_request_response_logging_config\": {\n",
    "        \"enabled\": True,\n",
    "        \"sampling_rate\": 1,\n",
    "        \"bigquery_destination\": {\n",
    "            \"output_uri\": OUTPUT_URI\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the PATCH request\n",
    "response = requests.patch(url, headers=headers, data=json.dumps(body))\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fd3ce",
   "metadata": {},
   "source": [
    "## Generate request traffic to our endpoint\n",
    "\n",
    "In this case, we're going to create a set of 5000 test requests from our original X_test dataset that we created at the beginning of this notebook, and we will then send those requests to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2343347",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "larger_test_instances = [x.tolist() for x in X_test[:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68559438",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store responses\n",
    "responses = []\n",
    "\n",
    "# Generate some predictions\n",
    "for instance in larger_test_instances:\n",
    "    response = endpoint.predict([instance])\n",
    "    \n",
    "    # Append the response to the list\n",
    "    responses.append(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d427e0",
   "metadata": {},
   "source": [
    "## Print out the first 10 predictions\n",
    "\n",
    "Let's take a look to see a sample of the responses provided by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39608e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the first 10 predictions\n",
    "print(\"Prediction results:\", responses[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a927e95",
   "metadata": {},
   "source": [
    "## View a prediction response in more detail\n",
    "\n",
    "Let's take a look at some of the other fields that are sent back in the reponses to our prediction requests.\n",
    "Note that we can see which model served the request by viewing the *deployed_model_id* field in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b03682",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the last response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebd907",
   "metadata": {},
   "source": [
    "## Perform analysis on the responses from our model\n",
    "\n",
    "In this case, we'll just perform a simple analysis that shows what percentage of the requests were served by each model, but BigQuery also provides the ability to perform much more complex analytics use cases.\n",
    "Considering the traffic_split configuration we specified, what would you expect the results to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3872802",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set our query with variables\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  deployed_model_id,\n",
    "  COUNT(*) AS response_count,\n",
    "  ROUND(COUNT(*) * 100 / SUM(COUNT(*)) OVER (), 2) AS percentage\n",
    "FROM\n",
    "  `{log_table_ref_id}`\n",
    "GROUP BY\n",
    "  deployed_model_id\n",
    "ORDER BY\n",
    "  percentage DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(sql)\n",
    "\n",
    "# Wait for the job to complete and get the result\n",
    "result = query_job.result()\n",
    "\n",
    "# Print the result\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37598c1",
   "metadata": {},
   "source": [
    "You should see that approximately 90% of the requests were served by our first model, and approximately 10% of the requests were served by our second model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b179de4",
   "metadata": {},
   "source": [
    "# Model monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e135e3",
   "metadata": {},
   "source": [
    "We can use Vertex AI Model Monitoring to inspect the performance of our models in much more detail, and to periodically monitor for any degredation in our model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb78cb",
   "metadata": {},
   "source": [
    "## Create BigQuery dataset and table to store training data for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b797d86",
   "metadata": {},
   "source": [
    "In order to detect this type of skew, we generally need to have access to the original training data, because Vertex AI Model Monitoring will compare the distribution of the training data against what is seen in the inference requests that are sent to our model in production.\n",
    "\n",
    "We'll put a copy of our training data from earlier in this notebook into BigQuery for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae2161",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your dataset ID\n",
    "train_dataset_id = \"cpt10_california_housing_dataset\"\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create a BigQuery dataset\n",
    "train_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{train_dataset_id}\")\n",
    "train_dataset = client.create_dataset(train_dataset)  # API request\n",
    "print(f\"Created dataset {PROJECT_ID}.{train_dataset_id}\")\n",
    "\n",
    "# Define your BigQuery table ID\n",
    "train_table_id = f\"{PROJECT_ID}.{train_dataset_id}.california_housing\"\n",
    "\n",
    "# Define the schema of your BigQuery table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"MedInc\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"HouseAge\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"AveRooms\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"AveBedrms\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"Population\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"AveOccup\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"Latitude\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"Longitude\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"MedHouseVal\", \"FLOAT64\")\n",
    "]\n",
    "\n",
    "# Create a BigQuery table\n",
    "train_table = bigquery.Table(train_table_id, schema=schema)\n",
    "train_table = client.create_table(train_table)  # API request\n",
    "\n",
    "print(f\"Created table {train_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0b97a",
   "metadata": {},
   "source": [
    "## Prepare training dataset to be put into BigQuery\n",
    "\n",
    "We can use our scaled housing dataset (data_scaled) that we created earlier in this notebook, which was used to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89066c76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use our scaled dataset that we created earlier in this notebook\n",
    "df = pd.DataFrame(data=data_scaled, columns=housing.feature_names)\n",
    "df['target'] = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6c54c",
   "metadata": {},
   "source": [
    "## Install pandas-gbq\n",
    "\n",
    "We can use the pandas-gbq library to make it easy for use to put our pandas dataframe data into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8832ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas-gbq --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba018d45",
   "metadata": {},
   "source": [
    "## Put training dataset in BigQuery\n",
    "\n",
    "We can use our scaled housing dataset (data_scaled) that we created earlier in this notebook, which was used to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4561081",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas_gbq import to_gbq\n",
    "\n",
    "# Upload the DataFrame to BigQuery\n",
    "to_gbq(df, train_table_id, project_id=PROJECT_ID, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f3eb8",
   "metadata": {},
   "source": [
    "## Create Vertex AI Model Monitoring Job\n",
    "\n",
    "Now it's time to create our Vertex AI Model Monitoring job.\n",
    "\n",
    "Some of the code in the following cell is repurposed from [this example notebook](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb#scrollTo=-62TYm2iYv3K). \n",
    "\n",
    "The output from the cell will contain a link at which you can view the details of the Vertex AI Model Monitoring Job. However, note that the job does not get created immediately, and therefore it may take some time for it to show up in the console. \n",
    "\n",
    "**You will receive an email at the email address you specify you in the code below, informing you about the status of the Vertex AI Model Monitoring Job.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1be3af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "USER_EMAIL=\"example@gmail.com\" # REPLACE WITH YOUR EMAIL\n",
    "\n",
    "JOB_NAME = \"cpt10-housing_monitoring_job\"\n",
    "\n",
    "# Sampling rate (optional, default=.8)\n",
    "LOG_SAMPLE_RATE = 0.9 \n",
    "\n",
    "# Monitoring Interval in hours (optional, default=1).\n",
    "MONITOR_INTERVAL = 1  \n",
    "\n",
    "# URI to training dataset.\n",
    "DATASET_BQ_URI = f\"bq://{train_table_id}\"  \n",
    "# Prediction target column name in training dataset.\n",
    "TARGET = \"target\"\n",
    "\n",
    "# # Skew and drift thresholds.\n",
    "\n",
    "DEFAULT_THRESHOLD_VALUE = 0.001\n",
    "\n",
    "SKEW_THRESHOLDS = {\n",
    "    \"MedInc\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"HouseAge\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveRooms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveBedrms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Population\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveOccup\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Latitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Longitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "}\n",
    "\n",
    "DRIFT_THRESHOLDS = {\n",
    "    \"MedInc\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"HouseAge\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveRooms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveBedrms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Population\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveOccup\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Latitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Longitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "}\n",
    "\n",
    "ATTRIB_SKEW_THRESHOLDS = {\n",
    "    \"MedInc\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"HouseAge\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveRooms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveBedrms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Population\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveOccup\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Latitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Longitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "}\n",
    "\n",
    "ATTRIB_DRIFT_THRESHOLDS = {\n",
    "    \"MedInc\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"HouseAge\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveRooms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveBedrms\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Population\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"AveOccup\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Latitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "    \"Longitude\": DEFAULT_THRESHOLD_VALUE,\n",
    "}\n",
    "\n",
    "skew_config = model_monitoring.SkewDetectionConfig(\n",
    "    data_source=DATASET_BQ_URI,\n",
    "    skew_thresholds=SKEW_THRESHOLDS,\n",
    "    attribute_skew_thresholds=ATTRIB_SKEW_THRESHOLDS,\n",
    "    target_field=TARGET,\n",
    ")\n",
    "\n",
    "drift_config = model_monitoring.DriftDetectionConfig(\n",
    "    drift_thresholds=DRIFT_THRESHOLDS,\n",
    "    attribute_drift_thresholds=ATTRIB_DRIFT_THRESHOLDS,\n",
    ")\n",
    "\n",
    "objective_config = model_monitoring.ObjectiveConfig(\n",
    "    skew_config, drift_config\n",
    ")\n",
    "\n",
    "# Create sampling configuration\n",
    "random_sampling = model_monitoring.RandomSampleConfig(sample_rate=LOG_SAMPLE_RATE)\n",
    "\n",
    "# Create schedule configuration\n",
    "schedule_config = model_monitoring.ScheduleConfig(monitor_interval=MONITOR_INTERVAL)\n",
    "\n",
    "# Create alerting configuration.\n",
    "emails = [USER_EMAIL]\n",
    "alerting_config = model_monitoring.EmailAlertConfig(\n",
    "    user_emails=emails, enable_logging=True\n",
    ")\n",
    "\n",
    "# Create the monitoring job.\n",
    "mon_job = aiplatform.ModelDeploymentMonitoringJob.create(\n",
    "    display_name=JOB_NAME,\n",
    "    logging_sampling_strategy=random_sampling,\n",
    "    schedule_config=schedule_config,\n",
    "    alert_config=alerting_config,\n",
    "    objective_configs=objective_config,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    endpoint=endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35aa55-8b83-4f3f-bf02-e4d4d1b00622",
   "metadata": {},
   "source": [
    "# Getting monitoring outputs\n",
    "\n",
    "**After you receive an email** telling you that the monitoring configuration has been set up, we will generate additonal traffic to our endpoint and view the related outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c8c85-4988-485d-8576-fefe2202cf76",
   "metadata": {},
   "source": [
    "## Generate request traffic to our endpoint\n",
    "\n",
    "Just as we did previously in this notebook, we're going send 5000 test requests from our original `X_test` dataset (that we created at the beginning of this notebook) to our endpoint. This activity will generate monitoring outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa615b84-8807-49de-a9e4-f7b106fc3e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store responses\n",
    "responses = []\n",
    "\n",
    "# Generate some predictions\n",
    "for instance in larger_test_instances:\n",
    "    response = endpoint.predict([instance])\n",
    "    \n",
    "    # Append the response to the list\n",
    "    responses.append(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ef435-dc4a-4c6c-a17a-154c495a3c69",
   "metadata": {},
   "source": [
    "## View monitoring outputs\n",
    "\n",
    "**The email you received will contain details and links regarding the monitoring job, including the BigQuery location at which the monitoring outputs are stored.**\n",
    "\n",
    "Next, let's move on to optimizing our model for edge deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5f15f",
   "metadata": {},
   "source": [
    "# Optimizing for Edge deployment\n",
    "\n",
    "Finally, let's optimize our model to be deployed at the edge. We're going to use TensorFlow Lite for that purpose.\n",
    "\n",
    "The following code will convert our model to TensorFlow Lite format, which is a light-weight format that is optimized for devices with limited computing resources. It will then save the converted model locally, after which we can upload it to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c5079",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "\n",
    "TF_MODEL_NAME = 'housing_model.tflite'\n",
    "\n",
    "# Convert the model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model\n",
    "with tf.io.gfile.GFile(TF_MODEL_NAME, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c3ebe",
   "metadata": {},
   "source": [
    "## Copy converted model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3aace",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $TF_MODEL_NAME $MODEL_LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3b88d",
   "metadata": {},
   "source": [
    "Now we have used TensorFlow Lite to optimize our model and we then stored the optimized model in Google Cloud Storage. From there, we can easily deploy our model to any device that supports the TensorFlow Lite interpreter. A list of supported platforms is provided in the TensorFlow Lite [documentation](https://www.tensorflow.org/lite/guide/inference#supported_platforms), which also contains lots of useful information on how TensorFlow Lite works in great detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabaae8-0dfc-4fd5-97a4-959944a7fc72",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb76c1-7a07-4d89-ada7-21245c7be5ba",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefbb9b-3110-4a1f-8e5b-6655568efd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74595a-473b-4894-8b43-840032bc1e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete Vertex AI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab384da3-1882-46b3-9b3e-deef73ebf5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    # Delete batch prediction job\n",
    "    try:\n",
    "        job.delete()\n",
    "        print(f\"Deleted Batch Prediction Job: {job.resource_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Batch Prediction Job: {e}\")\n",
    "\n",
    "    # Delete the monitoring job\n",
    "    try:\n",
    "        mon_job.delete()\n",
    "        print(f\"Deleted Monitoring Job: {mon_job.resource_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Batch Prediction Job: {e}\")\n",
    "        \n",
    "    # Delete endpoint\n",
    "    try:\n",
    "        endpoint_list = aiplatform.Endpoint.list(filter=f'display_name=\"{ENDPOINT_NAME}\"')\n",
    "        if endpoint_list:\n",
    "            endpoint = endpoint_list[0]  # Assuming only one endpoint with that name\n",
    "\n",
    "            # Undeploy all models (if any)\n",
    "            try:\n",
    "                endpoint.undeploy_all()\n",
    "                print(f\"Undeployed all models from endpoint: {ENDPOINT_NAME}\")\n",
    "            except exceptions.NotFound:\n",
    "                print(f\"No models found to undeploy from endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:  # Catching general errors for better debugging\n",
    "                print(f\"Unexpected error while undeploying models: {e}\")\n",
    "\n",
    "            # Delete endpoint\n",
    "            try:\n",
    "                endpoint.delete()\n",
    "                print(f\"Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "        else:\n",
    "            print(f\"Endpoint not found: {ENDPOINT_NAME}\")\n",
    "\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Endpoint not found: {ENDPOINT_NAME}\")\n",
    "\n",
    "    # Delete models\n",
    "    try:\n",
    "        model_list = aiplatform.Model.list(filter=f'display_name=\"{MODEL_NAME}\"')\n",
    "        if model_list:\n",
    "            for model in model_list:\n",
    "                print(f\"Deleting model: {model.display_name}\")\n",
    "                model.delete()\n",
    "        else:\n",
    "            print(f\"No models found matching: {MODEL_NAME}\")\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Model not found: {MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        model_100_list = aiplatform.Model.list(filter=f'display_name=\"{MODEL_100_NAME}\"')\n",
    "        if model_100_list:\n",
    "            for model in model_100_list:\n",
    "                print(f\"Deleting model: {model.display_name}\")\n",
    "                model.delete()\n",
    "        else:\n",
    "            print(f\"No models found matching: {MODEL_100_NAME}\")\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Model not found: {MODEL_100_NAME}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7da1-b4de-494d-9130-b62d53041a18",
   "metadata": {},
   "source": [
    "## Delete BigQuery tables and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc0c71-2676-4dc1-8e4f-c8262193f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    try:\n",
    "        client.delete_table(log_table_ref_id, not_found_ok=True)\n",
    "        print(f\"Deleted table {log_table_ref_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting table: {e}\")\n",
    "    try:\n",
    "        client.delete_table(train_table_id, not_found_ok=True)\n",
    "        print(f\"Deleted table {train_table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting table: {e}\")\n",
    "    try:\n",
    "        client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(f\"Deleted dataset: {dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset: {e}\")\n",
    "    try:\n",
    "        client.delete_dataset(train_dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(f\"Deleted dataset: {train_dataset_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2f8a5-4748-4be8-bc1e-f5007f00d471",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2e150-d910-415e-add3-b8274ee766da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca201-3eaa-4e13-b92a-6b8902df7ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
