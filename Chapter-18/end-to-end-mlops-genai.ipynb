{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end MLOps & Gen AI Event-driven Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background / overview\n",
    "\n",
    "In this notebook, we will build end-to-end event-driven solution that combines MLOps & Gen AI.\n",
    "\n",
    "We will focus on a computer vision (CV) use case to identify objects in images. Extending the CV use case we implemented in Chapter 15, we will build our CV model via an MLOps pipeline that incorporates the majority of the main topics from the earlier chapters in this book, such as data preparation, model training, deployment, inference, and evaluation. \n",
    "\n",
    "The novel approach we are using here is to utilize generative AI to generate the data that will be used to test and evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
    "* [Cloud Functions Pricing](https://cloud.google.com/functions/pricing)\n",
    "* [Eventarc Pricing](https://cloud.google.com/eventarc/pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Architecture\n",
    "\n",
    "Let's begin with the MLOps pipeline architecture.\n",
    "\n",
    "### MLOps pipeline architecture\n",
    "\n",
    "The following is the architecture of the MLOps pipeline we will build in this notebook:\n",
    "\n",
    "![MLOps](images/cpt-18-mlops-pipeline.png)\n",
    "\n",
    "In the MLOps pipeline, the process works as follows: \n",
    "\n",
    "1. The first step in our pipeline—the model training step—is invoked. While the MLOps pipeline we built for our tabular Titanic dataset in Chapter 11 started with distinct data preprocessing steps using Serverless Spark in Dataproc, in our pipeline in this chapter, the data ingestion and preparation steps are handled directly in the code of our model training job. Also, as noted, in this case, we are using the built-in CIFAR-10 image dataset in Tensorflow/Keras rather than fetching a dataset from an external source. Vertex AI Pipelines starts the model training process by submitting a model training job to the Vertex AI Training service.  \n",
    "\n",
    "1. In order to execute our custom training job, the Vertex AI Training service fetches our custom Docker container from Google Artifact Registry. \n",
    "\n",
    "1. When our model has been trained, the trained model artifacts are saved in Google Cloud Storage.  \n",
    "\n",
    "1. The model training job status is complete.  \n",
    "\n",
    "1. The next step in our pipeline—the model import step—is invoked. This is an intermediate step that prepares the model metadata to be referenced in later components of our pipeline. The relevant metadata in this case consists of the location of the model artifacts in Google Cloud Storage and the specification of the Docker container image in Google Artifact Registry that will be used to serve our model.  \n",
    "\n",
    "1. The next step in our pipeline—the model upload step—is invoked. This step references the metadata from the model import step.  \n",
    "\n",
    "1. The model metadata is used to register the model in Vertex AI Model Registry. This makes it easy to deploy our model for serving traffic in Vertex AI.  \n",
    "\n",
    "1. The model upload job status is complete.  \n",
    "\n",
    "1. The next step in our pipeline—the endpoint creation step—is invoked.  \n",
    "\n",
    "1. An endpoint is created in the Vertex AI Prediction service. This endpoint will be used to host our model.  \n",
    "\n",
    "1. The endpoint creation job status is complete.  \n",
    "\n",
    "1. The next step in our pipeline - the model deployment step - is invoked.  \n",
    "\n",
    "1. Our model is deployed to our endpoint in the Vertex AI Prediction service. This step references the metadata of the endpoint that has just been created by our pipeline, as well as the metadata of our model in the Vertex AI Model Registry.   \n",
    "\n",
    "1. The model deployment job status is complete.  \n",
    "\n",
    "### End-to-end solution architecture\n",
    "\n",
    "The solution architecture is shown in the following diagram:\n",
    "\n",
    "![end-to-end](images/cpt-18-end-to-end.png)\n",
    "\n",
    "Our MLOps pipeline is simplified in the top-left corner of the diagram. It still implements all of the same steps we discussed in the previous section, but the diagram is simplified so we can focus our discussion on the broader, end-to-end solution. In this context, the MLOps pipeline is represented as a single step in the overall process. \n",
    "\n",
    "The following set of steps describes the architecture:\n",
    "\n",
    "1. Our MLOps pipeline trains and deploys our CV model. \n",
    "\n",
    "1. When the MLOps pipeline completes, it publishes a message to a Pub/Sub topic we created for that purpose. \n",
    "\n",
    "1. Eventarc detects that a message has been published to the Pub/Sub topic. \n",
    "\n",
    "1. Eventarc triggers the Cloud Function we’ve created to generate an image. \n",
    "\n",
    "1. The code in our image generation function makes a call to the Imagen API with a prompt to generate an image containing one of the types of objects our model was trained to recognize (i.e., a type of object supported by the CIFAR-10 dataset). \n",
    "\n",
    "1. Imagen generates an image and returns it to our function. \n",
    "\n",
    "1. Our function stores the new image in GCS. \n",
    "\n",
    "1. GCS emits an event indicating that a new object has been uploaded to our bucket. Eventarc detects this event. \n",
    "\n",
    "1. Eventarc invokes our next Cloud Function and passes the GCS event metadata to our function. This metadata includes details such as the identifiers of the bucket and the object in question. \n",
    "\n",
    "1. Our prediction function takes the details regarding the bucket and the object in question from the event metadata and uses those details to fetch the newly created object (i.e., the newly generated image from Imagen). \n",
    "\n",
    "1. Our prediction function then performs some preprocessing on the image to transform it into a format that is expected by our model (i.e., similar to the format of the CIFAR-10 data the model was trained on). Our function then sends the transformed data as a prediction request to the Vertex AI endpoint that hosts our model. \n",
    "\n",
    "1. Our model predicts what type of object is in the image, and sends a prediction response to our Cloud Function. \n",
    "\n",
    "1. Our Cloud Function saves the prediction response in GCS. \n",
    "\n",
    "When the process has been completed, you can view the generated image and the resulting prediction from our model in GCS. \n",
    "\n",
    "Notice that all of the steps in the solution are implemented automatically and without the need to provision any servers. This is a fully serverless, event-driven solution architecture. \n",
    "\n",
    "An interesting side-effect of this solution is that, although the primary intention is to test our newly trained model on generated data, this solution could also be applied to the inverse use case. That is, if we are confident that our model has been trained effectively and provides consistently accurate results, we could use it to evaluate the quality of the generated data. For example, if our model predicts that the generated data contains a particular type of object with a probability of 99.8%, we can interpret this as a reflection of the quality of the generated data. \n",
    "\n",
    "Now that we’ve discussed the various steps in the process, let’s start building it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Initial setup\n",
    "\n",
    "In this section, we set up all of the baseline requirements to build our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "We will use the following libraries in this notebook:\n",
    "\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview/)\n",
    "* [Google Cloud Pipeline Components (GCPC)](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Sometimes the `pip` installation commands display warnings or errors regarding dependencies. In Chapter 14, I explain how to create custom Conda kernels to avoid dependency conflicts. However, if you have been following the instructions in the book that accompanies this repo (creating new Vertex AI notebooks where relevant, etc.) then our activities are not affected by any dependency conflicts, and you can ignore any pip dependency errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --quiet --user --upgrade google-cloud-aiplatform kfp google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines (KFP)\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, Input, Output, Artifact\n",
    "\n",
    "# Google Cloud Pipeline Components (GCPC)\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable services\n",
    "\n",
    "We will use the following Google Cloud services and APIs in this solution:\n",
    "* [Cloud Functions](https://cloud.google.com/functions?hl=en)\n",
    "* [Cloud Run](https://cloud.google.com/run?hl=en) (Cloud Functions run on Cloud Run)\n",
    "* [Eventarc](https://cloud.google.com/eventarc/docs)\n",
    "* [Pub/Sub](https://cloud.google.com/pubsub?hl=en)\n",
    "\n",
    "In order to use those services, we need to enable their APIs in our GCP project. The following command enables them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services enable cloudfunctions.googleapis.com run.googleapis.com eventarc.googleapis.com pubsub.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that they are enabled\n",
    "\n",
    "The following command lists all enabled APIs in our GCP project, and filters for the ones we want to use in this solution. Ensure that each of the relevant APIs appears in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services list --enabled | egrep 'functions|run|event|pub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "In this section, we define all of the constants that will be referenced throughout the rest of the notebook.\n",
    "\n",
    "**REPLACE THE REGION, AND BUCKET DETAILS WITH YOUR DETAILS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQOV9ssPWbMB"
   },
   "outputs": [],
   "source": [
    "# Core constants\n",
    "BUCKET_URI = f\"gs://{BUCKET}\"\n",
    "BUCKET_DIR = \"chapter-18\"\n",
    "APPLICATION_DIR = \"mlops-images-app\" # Local parent directory for our pipeline resources\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\" # Local directory for training resources\n",
    "APP_NAME=\"mlops-images\" # Base name for our pipeline application\n",
    "\n",
    "# Cloud Function constants\n",
    "GEN_FUNCTION=\"image_gen_function\"\n",
    "PREDICT_FUNCTION=\"image_predict_function\"\n",
    "DATA_FOLDER_PATH = f\"{BUCKET_DIR}-data\"\n",
    "PRED_FOLDER_PATH = f\"{BUCKET_DIR}-predictions\"\n",
    "PROMPT = \"A ship on the ocean. It is fully visible.\" # We can use any object from CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "TOPIC_NAME=\"pipeline_completed_notifications\"\n",
    "\n",
    "# Pipeline constants\n",
    "PIPELINE_NAME = \"mlops-images-pipeline\" # Name of our pipeline\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\" # (See: https://www.kubeflow.org/docs/components/pipelines/v1/overview/pipeline-root/)\n",
    "MODEL_NAME = \"mlops-images\" # Name of our model\n",
    "EXPERIMENT_NAME = \"aiml-sa-images-experiment\" # Vertex AI \"Experiment\" name for metadata tracking\n",
    "\n",
    "# Training constants\n",
    "TRAIN_REPO_NAME=f'{APP_NAME}-training' # Name of repository in which we will store our custom training image\n",
    "TRAIN_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{TRAIN_REPO_NAME}/{APP_NAME}-train:latest\"\n",
    "MODEL_URI = f\"{BUCKET_URI}/models/chapter-18/cv\" # Where to store our trained model\n",
    "\n",
    "# Hyperparameters for training\n",
    "BATCH_SIZE: int = 4\n",
    "EPOCHS: int = 30\n",
    "LEARNING_RATE: float = 0.001\n",
    "\n",
    "# Arguments to pass to our training job\n",
    "TRAINING_ARGS=[\n",
    "    \"--project_id\",\n",
    "    PROJECT_ID,\n",
    "    \"--bucket_name\",\n",
    "    BUCKET,\n",
    "    \"--model_path\",\n",
    "    MODEL_URI,\n",
    "    \"--batch_size\",\n",
    "    str(BATCH_SIZE),\n",
    "    \"--epochs\",\n",
    "    str(EPOCHS),\n",
    "    \"--learning_rate\",\n",
    "    str(LEARNING_RATE),\n",
    "]\n",
    "\n",
    "# Worker pool spec (see https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec)\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"args\": TRAINING_ARGS\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Serving constants\n",
    "SERVING_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\" # (See: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)\n",
    "ENDPOINT_NAME = \"mlops-endpoint\" # Name of endpoint on which to serve our trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local directories\n",
    "We will use the following local directories during the activities in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a source directory to save the code\n",
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR\n",
    "!mkdir -p $GEN_FUNCTION\n",
    "!mkdir -p $PREDICT_FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set  project ID for  gcloud\n",
    "The following command sets our project ID for using gcloud commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create $TOPIC_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vertex AI SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Cloud Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the image generation function\n",
    "\n",
    "First, we'll create the function that will use Imagen to generate an image. Our function stores the image in GCS (we'll write another function later that will take our newly-generated image from GCS and send it in an inference request to a Computer Vision model that we will train later in this notebook).\n",
    "\n",
    "In this section we are not directly executing the code in this notebook; we are writing/saving the code to local files that will be uploaded to be executed by Google Cloud Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "The requirements.txt file is a convenient way to specify all of the packages that we want to install in our custom container image. This file will be referenced in the Dockerfile for our image.\n",
    "\n",
    "In this case, we will install:\n",
    "* [Google Cloud Functions Framework](https://cloud.google.com/functions/docs/functions-framework)\n",
    "* [The Vertex Generative AI Python SDK](https://pypi.org/project/vertexai/)\n",
    "* [Pillow](https://pypi.org/project/pillow/)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {GEN_FUNCTION}/requirements.txt\n",
    "functions-framework==3.*\n",
    "vertexai\n",
    "Pillow\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our function code \n",
    "\n",
    "This is the code our function will run when invoked. It will perform the following steps:\n",
    "1. Import required libraries.\n",
    "1. Set local variables from [environment variables](https://cloud.google.com/functions/docs/configuring/env-var). (We specified the values of these variables earlier in this notebook.)\n",
    "1. Implement a function that is invoked by Google Cloud Events (this is specified by using the `@functions_framework.cloud_event` decorator). This function does the following:\n",
    "* Set up the Vertex AI environment using our specified project and region.\n",
    "* Load the Imagen `\"imagegeneration@006\"` image generation model.\n",
    "* Generate an image. See [documentation here](https://cloud.google.com/vertex-ai/generative-ai/docs/image/generate-images#vertex-ai-sdk-for-python) for details regarding the specified parameters and values.\n",
    "* Store the image in GCS.\n",
    "* Return the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {GEN_FUNCTION}/main.py\n",
    "import functions_framework\n",
    "from PIL import Image\n",
    "import vertexai\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Environment variables\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "REGION = os.environ.get(\"REGION\")\n",
    "BUCKET = os.environ.get(\"BUCKET\")\n",
    "DATA_FOLDER_PATH = os.environ.get(\"DATA_FOLDER_PATH\")\n",
    "PROMPT = os.environ.get(\"PROMPT\")\n",
    "\n",
    "BLOB_NAME = \"func_generated_image.png\"\n",
    "\n",
    "@functions_framework.cloud_event  # Decorator for Cloud Events \n",
    "def generate_and_store_image(cloud_event):\n",
    "    try:\n",
    "        # Initialize the Vertex AI environment using our specified project and region\n",
    "        vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "        \n",
    "        # Load the Imagen `\"imagegeneration@006\"` image generation model\n",
    "        model = ImageGenerationModel.from_pretrained(\"imagegeneration@006\")\n",
    "\n",
    "        # Generate an image\n",
    "        images = model.generate_images(\n",
    "            prompt=PROMPT,\n",
    "            number_of_images=1,\n",
    "            language=\"en\",\n",
    "            aspect_ratio=\"1:1\",\n",
    "            safety_filter_level=\"block_some\",\n",
    "            person_generation=\"allow_adult\",\n",
    "        )\n",
    "\n",
    "        # Store the image data in GCS\n",
    "        image_data = images[0]._image_bytes\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        bucket = storage_client.bucket(BUCKET)\n",
    "        blob_path = f\"{DATA_FOLDER_PATH}/{BLOB_NAME}\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_string(image_data, content_type=\"image/png\")\n",
    "\n",
    "        # Return status\n",
    "        return \"Image generated and stored successfully!\", 200\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return f\"An error occurred: {e}\", 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the image generation function\n",
    "\n",
    "Until this point, we have simply saved our function code locally in our Jupyter Notebook. In this section, we will deploy our code to the Google Cloud Functions service, and specify the trigger that will cause the function to be invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will use the `gcloud functions deploy` command to deploy our Cloud Function. The command will perform all of the following steps on our behalf:\n",
    "\n",
    "1. Packages the code and any dependencies to be deployed to Cloud Functions. (This consists of the `main.py` and `requirements.txt` files we created above.)\n",
    "1. Triggers Cloud Build to build a container image for our function. The build process includes:\n",
    "* Fetching the base container image based on our chosen runtime (in our case, Python 3.12).\n",
    "* Copying our function code into the container.\n",
    "* Installing dependencies (as specified in the `requirements.txt` file).\n",
    "* Configuring the function entry point (the function to be executed when the Cloud Function is triggered).\n",
    "\n",
    "The resulting container image is stored in Google Artifact Registry.\n",
    "\n",
    "The variables and flags in the command are as follows:\n",
    "* `{GEN_FUNCTION}`: The name of the Cloud Function to deploy. \n",
    "* `--region {REGION}`: The region in which to deploy our Cloud Function.\n",
    "* `--runtime python312`: Our desired function runtime version; in this case, Python 3.12\n",
    "* `--memory 512`: The desired amount of memory to use for running our function; in this case, 512 MB\n",
    "* `--trigger-topic {TOPIC_NAME}`: The `--trigger-topic` flag specifies that we want our function to be triggered every time a message is published to a specific Pub/Sub topic. The `{TOPIC_NAME}` specifies the name of the topic.\n",
    "* `--entry-point generate_and_store_image`: Specifies the function within our code that should be executed on each invocation. (In this case, it's our `generate_and_store_image` function.)\n",
    "* `--source {GEN_FUNCTION}`: The name of the local directory in our Jupyter Notebook that contains the required code files.\n",
    "* `--gen2`: This specifies that we want to deploy a 2nd-generation Cloud Function. See further details [here](https://cloud.google.com/functions/docs/concepts/version-comparison).\n",
    "* `--no-allow-unauthenticated`: Require that the request is authenticated (i.e., do not allow unauthenticated requests).\n",
    "* `--set-env-vars`: This allows us to set environment variables. See further details [here](https://cloud.google.com/functions/docs/configuring/env-var).\n",
    "\n",
    "Note: we use the `%%capture output` Jupyter magic to capture the output because the build process generates a lot of output messages. See further details [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "! gcloud functions deploy {GEN_FUNCTION} --region {REGION} --runtime python312 --memory 512 --trigger-topic {TOPIC_NAME} --entry-point generate_and_store_image --source {GEN_FUNCTION} --gen2 --no-allow-unauthenticated --set-env-vars \"PROJECT_ID={PROJECT_ID},REGION={REGION},DATA_FOLDER_PATH={DATA_FOLDER_PATH},BUCKET={BUCKET},PROMPT={PROMPT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on Eventarc\n",
    "\n",
    "In our solution, Eventarc is used behind the scenes to detect our trigger events and to invoke our functions. Eventarc is designed to capture events from various Google Cloud services and route them to appropriate destinations. In this case, we do not need to explicitly configure Eventarc triggers because they will automatically be set up as follows:\n",
    "\n",
    "* When we deploy our Cloud Function with the `--trigger-topic` flag in the `gcloud functions deploy` command, we're telling Google Cloud to invoke our function every time a message is published to the specified topic. \n",
    "\n",
    "* Behind the scenes, Google Cloud sets up an Eventarc trigger that listens for messages being published to your Pub/Sub topic.\n",
    "\n",
    "* When a message is published to the relevant Pub/Sub topic, Eventarc's trigger detects it and automatically invokes our Cloud Function, passing the Pub/Sub message data as an argument to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the function has been deployed successfully\n",
    "\n",
    "The following command checks the logs of the Cloud Function we deployed. If all went well, you should just see a few lines that show the function startup messages; you should not see the word \"error\"\n",
    "\n",
    "You can also check the deployed Cloud Function details by navigating to `Cloud Functions` in the Google Cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud functions logs read {GEN_FUNCTION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the image prediction function\n",
    "\n",
    "Next, we'll create the function that will take the image generated by our previous function and send it to our Computer Vision model (which we will train later in this notebook). \n",
    "\n",
    "Again, in this section we are not directly executing the code in this notebook; we are writing/saving the code to local files that will be uploaded to be executed by Google Cloud Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "Just as we did for our image generation function above, we need to create a requirements.txt file that specifies all of the dependencies that need to be installed for our function to execute correctly. \n",
    "\n",
    "In this case, we will install:\n",
    "* [Google Cloud Functions Framework](https://cloud.google.com/functions/docs/functions-framework)\n",
    "* [Pillow](https://pypi.org/project/pillow/)\n",
    "* [NumPy](https://numpy.org/)\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {PREDICT_FUNCTION}/requirements.txt\n",
    "functions-framework==3.*\n",
    "Pillow\n",
    "numpy\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our function code \n",
    "\n",
    "This is the code our function will run when invoked. It will perform the following steps:\n",
    "1. Import required libraries.\n",
    "1. Set local variables from [environment variables](https://cloud.google.com/functions/docs/configuring/env-var). (We specified the values of these variables earlier in this notebook.)\n",
    "1. Create a GCS client that will be used to fetch and write data from/to GCS.\n",
    "1. Implement a function that is invoked by Google Cloud Events (this is specified by using the `@functions_framework.cloud_event` decorator). In this case, the event is a GCS event that is generated when an object is uploaded or changed in our specified bucket. This function does the following:\n",
    "* Extract data from the GCS event\n",
    "* Download an open the relevant image (this is the image that was generated by our previous image generation function).\n",
    "* Perform some image preprocessing steps. Remember that our Computer Vision model is trained on the CIFAR-10 dataset, so it expects to see images in that format. Our image generation function used Imagen to generate an image, and that image is not in the same format as the CIFAR-10 dataset, so we need to transform our image to a compatible format for our model. Specifically, we resize the image, convert it to RGB, then convert it to a Numpy array, and normalize the resulting array. At that point, it is ready to send to our Computer Vision model.\n",
    "* Prepare a prediction request payload for our Computer Vision model. \n",
    "* Get our Vertex AI endpoint resource name from the environment variable.\n",
    "* Send the inference request using the request payload.\n",
    "* Extract the predictions from the response and find the highest probability class.\n",
    "* Format the prediction results in JSON format. This is just an optional, standardized way to represent the outputs.\n",
    "* Store the prediction results in GCS.\n",
    "* Return the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {PREDICT_FUNCTION}/main.py\n",
    "import functions_framework\n",
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "REGION = os.environ.get(\"REGION\")\n",
    "ENDPOINT_NAME = os.environ.get(\"ENDPOINT_NAME\")\n",
    "DATA_FOLDER_PATH = os.environ.get(\"DATA_FOLDER_PATH\")\n",
    "PRED_FOLDER_PATH = os.environ.get(\"PRED_FOLDER_PATH\")\n",
    "\n",
    "# Create a GCS client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "@functions_framework.cloud_event \n",
    "def predict(cloud_event):\n",
    "    try:\n",
    "        # Extract data from the GCS event\n",
    "        data = cloud_event.data\n",
    "        bucket_name = data[\"bucket\"]\n",
    "        object_name = data[\"name\"]\n",
    "        image_path = f\"gs://{bucket_name}/{object_name}\"\n",
    "        \n",
    "        if not object_name.startswith(DATA_FOLDER_PATH):\n",
    "            print(f\"Skipping object outside of target folder: {object_name}\")\n",
    "            return \"Object not in target folder\", 200\n",
    "        else: \n",
    "            # Get a reference to the GCS bucket and blob\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(object_name)\n",
    "\n",
    "            # Download the image data as bytes\n",
    "            image_bytes = blob.download_as_bytes()\n",
    "\n",
    "            # Load the image using Pillow's Image.open\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "\n",
    "            # Image preprocessing \n",
    "            resized_image = image.resize((32, 32), resample=Image.BILINEAR)\n",
    "            rgb_image = resized_image.convert('RGB')\n",
    "            image_array = np.array(rgb_image)\n",
    "            normalized_image = image_array / 255.0\n",
    "\n",
    "            # Create the request payload\n",
    "            request_payload = {\n",
    "                \"instances\": [normalized_image.tolist()]  # Reshape to [32, 32, 3]\n",
    "            }\n",
    "\n",
    "            # Get the endpoint resource name\n",
    "            mlops_endpoint_list = aiplatform.Endpoint.list(\n",
    "                filter=f'display_name={ENDPOINT_NAME}', order_by='create_time desc'\n",
    "            )\n",
    "            new_mlops_endpoint = mlops_endpoint_list[0]\n",
    "            endpoint_resource_name = new_mlops_endpoint.resource_name\n",
    "            print(endpoint_resource_name)\n",
    "\n",
    "            # Send the inference request using the request payload\n",
    "            response = aiplatform.Endpoint(endpoint_resource_name).predict(\n",
    "                instances=request_payload[\"instances\"]\n",
    "            )\n",
    "\n",
    "            # Extract predictions and find the highest probability class\n",
    "            predictions = response.predictions[0]\n",
    "            class_index = predictions.index(max(predictions))\n",
    "            class_probability = max(predictions)\n",
    "\n",
    "            # CIFAR-10 class labels\n",
    "            class_labels = [\n",
    "                \"airplane\",\n",
    "                \"automobile\",\n",
    "                \"bird\",\n",
    "                \"cat\",\n",
    "                \"deer\",\n",
    "                \"dog\",\n",
    "                \"frog\",\n",
    "                \"horse\",\n",
    "                \"ship\",\n",
    "                \"truck\",\n",
    "            ]\n",
    "            predicted_label = class_labels[class_index]\n",
    "\n",
    "            # Format the prediction results in JSON format\n",
    "            prediction_result = json.dumps({\"Predicted class\": predicted_label, \"probability\": class_probability})\n",
    "\n",
    "            # Upload prediction results to GCS\n",
    "            result_blob_name = (\n",
    "                f\"{PRED_FOLDER_PATH}/{object_name}-prediction.txt\"  # Store results in a subfolder with the image name\n",
    "            )\n",
    "            result_blob = bucket.blob(result_blob_name)\n",
    "            result_blob.upload_from_string(prediction_result)\n",
    "\n",
    "            print(f\"Prediction results uploaded to: gs://{bucket_name}/{result_blob_name}\")\n",
    "\n",
    "            return prediction_result, 200\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return json.dumps({\"error\": str(e)}), 500  # Return error response with status code 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the image prediction function\n",
    "\n",
    "In the next cell, we will use the `gcloud functions deploy` command to deploy our image prediction Cloud Function. Again, the command will perform all of the following steps on our behalf:\n",
    "\n",
    "1. Packages the code and any dependencies to be deployed to Cloud Functions. (This consists of the `main.py` and `requirements.txt` files we created above.)\n",
    "1. Triggers Cloud Build to build a container image for our function. The build process includes:\n",
    "* Fetching the base container image based on our chosen runtime (in our case, Python 3.12).\n",
    "* Copying our function code into the container.\n",
    "* Installing dependencies (as specified in the `requirements.txt` file).\n",
    "* Configuring the function entry point (the function to be executed when the Cloud Function is triggered).\n",
    "\n",
    "Again, the resulting container image is stored in Google Artifact Registry.\n",
    "\n",
    "For the most part, the flags and variables used in the command are similar to the ones we used to deploy our image generation function earlier in this notebook. Apart from some slightly different environment variables (based on the needs of our function), the following is the main difference:`--trigger-bucket {BUCKET}`\n",
    "\n",
    "When we deploy a 2nd generation Cloud Function with the `--trigger-bucket` flag, we're specifying that we want the function to be triggered by events in a specific GCS bucket. In our case, we're specifying the bucket in the `{BUCKET}` variable.\n",
    "\n",
    "Again, Eventarc is automatically configured and used behind the scenes to handle the triggering mechanism. This trigger is configured to:\n",
    "* Listen for the specified event type (in our case, the `google.storage.object.finalize` event for object creation/finalization).\n",
    "* Filter events based on the bucket we provided.\n",
    "* Deliver the event data to our Cloud Function.\n",
    "\n",
    "When a matching event occurs in our GCS bucket (e.g., a new object is uploaded), Eventarc captures the event, filters it, and then invokes our Cloud Function, passing the event data as a parameter.\n",
    "\n",
    "Note: again, we use the `%%capture output` Jupyter magic to capture the output because the build process generates a lot of output messages. See further details [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-capture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "! gcloud functions deploy {PREDICT_FUNCTION} --region {REGION} --runtime python312 --trigger-bucket {BUCKET} --entry-point predict --source {PREDICT_FUNCTION} --gen2 --no-allow-unauthenticated --memory 512 --set-env-vars \"PROJECT_ID={PROJECT_ID},REGION={REGION},DATA_FOLDER_PATH={DATA_FOLDER_PATH},PRED_FOLDER_PATH={PRED_FOLDER_PATH},ENDPOINT_NAME={ENDPOINT_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the function has been deployed successfully\n",
    "\n",
    "The following command checks the logs of the Cloud Function we deployed. If all went well, you should just see a few lines that show the function startup messages; you should not see the word \"error\"\n",
    "\n",
    "You can also check the deployed Cloud Function details by navigating to `Cloud Functions` in the Google Cloud console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud functions logs read {PREDICT_FUNCTION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom training job\n",
    "In this section, we will create our custom training job to train our Computer Vision model. It will consist of the following steps:\n",
    "1. Create a Google Artifact Registry repository to host our custom container image.\n",
    "2. Create our custom training script.\n",
    "3. Create a Dockerfile that will specify how to build our custom container image. \n",
    "4. Build our custom container image.\n",
    "5. Push our custom container image to Google Artifact Registry so that we can use it in subsequent steps in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Google Artifact Registry repository\n",
    "\n",
    "Our custom training component in our pipeline will run in a container on the Vertex AI Training service. In this section, we will create the Google Artifact Registry repository in which we can store our custom container image that we will build in later steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $TRAIN_REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"Train repo for MLOps images workload\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "399eba3ab133"
   },
   "source": [
    "## Define the code for our training job\n",
    "\n",
    "The following code will create a file that contains the code for our custom training job. \n",
    "\n",
    "The code performs the following processing steps:\n",
    "\n",
    "1. Imports required libraries and sets initial variable values based on arguments passed to the script (the arguments are described below).\n",
    "2. Reads in and prepares the dataset.\n",
    "3. Trains our Keras Sequential convolutional neural network (CNN) model for Computer Vision.\n",
    "4. Saves the model artifacts to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_DIR}/train.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets, optimizers, utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "    \n",
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def train_model(args):\n",
    "    # Input arguments\n",
    "    project_id = args.project_id\n",
    "    bucket_name = args.bucket_name\n",
    "    model_path = args.model_path\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    \n",
    "    ### DATA PREPARATION SECTION ###\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0    \n",
    "    # Convert class vectors to binary class matrices\n",
    "    y_train = utils.to_categorical(y_train, 10)\n",
    "    y_test = utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    ### MODEL TRAINING AND EVALUATION SECTION ###\n",
    "\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        device = '/GPU:0'\n",
    "    else:\n",
    "        device = '/CPU:0'\n",
    "    \n",
    "    with tf.device(device):\n",
    "        net = create_model()\n",
    "\n",
    "    # Compile the model\n",
    "    net.compile(optimizer=optimizers.SGD(learning_rate=learning_rate, momentum=0.9),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train the network\n",
    "    history = net.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                      validation_data=(x_test, y_test))\n",
    "\n",
    "    # Save the trained model locally\n",
    "    net.save(model_path)\n",
    "    \n",
    "    # Return the trained model\n",
    "    return net  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train a CNN model on the CIFAR-10 dataset')\n",
    "    \n",
    "    parser.add_argument('--project_id', type=str, help='GCP Project ID')\n",
    "    parser.add_argument('--bucket_name', type=str, help='GCP Bucket ID')\n",
    "    parser.add_argument('--model_path', type=str, help='Path to save the trained model')\n",
    "    parser.add_argument('--batch_size', type=int, default=4, help='Batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='Number of epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the requirements.txt file\n",
    "Just as we did for our Cloud Functions above, we need to create a requirements.txt file that specifies all of the dependencies that need to be installed for our training code to execute correctly. \n",
    "\n",
    "In this case, we will install:\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)\n",
    "* [Filesystem interfaces for Python](https://filesystem-spec.readthedocs.io/en/latest/)\n",
    "* [GCSFS](https://gcsfs.readthedocs.io/en/latest/)\n",
    "* [pyarrow](https://arrow.apache.org/docs/python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/requirements.txt\n",
    "google-cloud-aiplatform\n",
    "tensorflow>=2.0.0\n",
    "numpy\n",
    "argparse\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the Docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our custom training image\n",
    "\n",
    "The steps required to build our image are:\n",
    "\n",
    "1. Change directory to our application directory.\n",
    "2. Build Docker image.\n",
    "3. Push the image to our Google Artifact Registry.\n",
    "4. Change directory back to our parent application directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $APPLICATION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build ./ -t $TRAIN_IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push our custom image to Google Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push $TRAIN_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define component to notify when pipeline completes\n",
    "\n",
    "When our pipeline completes, we will publish a message to the Pub/Sub topic we created earlier in this notebook. That event will cause our image generation Cloud Function to be invoked, which will then kick off the rest of the processes in our solution.\n",
    "\n",
    "The following pipeline component will publish the message to the Pub/Sub topic when our pipeline completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-pubsub\"], base_image=\"python:3.12\")\n",
    "def publish_message(project_id: str, topic_name: str, pipeline_run_id: str = None):\n",
    "    \"\"\"Publishes a message to a Pub/Sub topic with the pipeline run ID.\"\"\"\n",
    "    from google.cloud import pubsub_v1\n",
    "\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "    topic_path = publisher.topic_path(project_id, topic_name)\n",
    "    \n",
    "    if pipeline_run_id is None:\n",
    "        # Try to get the pipeline run ID from KFP environment variables\n",
    "        pipeline_task = kfp.dsl.get_current_task()\n",
    "        pipeline_run_id = pipeline_task.pipeline_run_id\n",
    "        print(f\"Fetched pipeline run ID from KFP environment: {pipeline_run_id}\")\n",
    "    else:\n",
    "        print(f\"Received pipeline run ID as argument: {pipeline_run_id}\")\n",
    "\n",
    "    # Create a message with the run ID\n",
    "    message = f\"Pipeline with run ID '{pipeline_run_id}' completed.\" \n",
    "\n",
    "    # Publish a message\n",
    "    message = \"Pipeline completed\"  # You can customize the message\n",
    "    data = message.encode(\"utf-8\")\n",
    "    future = publisher.publish(topic_path, data)\n",
    "    print(f\"Published message ID: {future.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQd9M1_9bif7"
   },
   "source": [
    "# Define our Vertex AI Pipeline\n",
    "\n",
    "Now it's time to define the sequence of steps in our MLOps pipeline.\n",
    "\n",
    "In this section, we will use the Kubeflow Pipelines SDK and Google Cloud Pipeline Components to define our MLOps pipeline.\n",
    "\n",
    "We begin by specifying all of the required variables in our pipeline, and populating their values from the constants we defined earlier in our notebook. We then specify the following components in our pipeline:\n",
    "\n",
    "1. [CustomTrainingJobOp](https://cloud.google.com/vertex-ai/docs/pipelines/customjob-component#customjobop) to perform our custom model training step.\n",
    "1. [importer](https://www.kubeflow.org/docs/components/pipelines/v2/components/importer-component/) to import our [UnmanagedContainerModel](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.UnmanagedContainerModel) object.\n",
    "1. [ModelUploadOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/model.html#v1.model.ModelUploadOp) to upload our Model artifact into Vertex AI Model Registry.\n",
    "1. [EndpointCreateOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.EndpointCreateOp) to create a Vertex AI [Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints).\n",
    "1. [ModelDeployOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.ModelDeployOp) to deploy our Google Cloud Vertex AI Model to an Endpoint, creating a [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) object within it.\n",
    "1. `publish_completion_message_op` to the message to our Pub/Sub topic when our pipeline completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX2u9hXDWtpp"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME, description=\"MLOps pipeline for custom data preprocessing, model training, and deployment.\")\n",
    "def pipeline(\n",
    "    bucket_name: str = BUCKET,\n",
    "    display_name: str = PIPELINE_NAME,\n",
    "    model_path: str = MODEL_URI,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    worker_pool_specs: list = WORKER_POOL_SPEC,\n",
    "    base_output_directory: str = PIPELINE_ROOT,\n",
    "    serving_image_uri: str = SERVING_IMAGE_URI,\n",
    "    endpoint_name: str = ENDPOINT_NAME,\n",
    "    topic_name: str = TOPIC_NAME\n",
    "):\n",
    "    \n",
    "    # Train model\n",
    "    model_training_op = custom_job.CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        display_name=\"train-mlops-model\",\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "    )\n",
    "    \n",
    "    importer_op = dsl.importer(\n",
    "        artifact_uri=model_path,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": serving_image_uri,\n",
    "            },\n",
    "        },\n",
    "    ).after(model_training_op)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=model_name,\n",
    "        unmanaged_container_model=importer_op.outputs[\"artifact\"],\n",
    "    ).after(importer_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=endpoint_name,\n",
    "    ).after(model_upload_op)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    ).after(endpoint_create_op)\n",
    "    \n",
    "    publish_completion_message_op = publish_message(\n",
    "        project_id=project_id, \n",
    "        topic_name=topic_name,\n",
    "        pipeline_run_id=\"{{workflow.uid}}\",\n",
    "    ).after(model_deploy_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ePytY8t3bu"
   },
   "source": [
    "### Compile our pipeline into a YAML file\n",
    "\n",
    "Now that we have defined out pipeline structure, we need to compile it into YAML format in order to run it in Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFyCaNPIWtsU"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, 'mlops-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq26zYhQb0qm"
   },
   "source": [
    "## Submit and run our pipeline in Vertex AI Pipelines\n",
    "\n",
    "Now we're ready to use the Vertex AI Python SDK to submit and run our pipeline in Vertex AI Pipelines.\n",
    "\n",
    "The parameters, artifacts, and metrics produced from the pipeline run are automatically captured into Vertex AI Experiments as an experiment run. We will discuss the concept of Vertex AI Experiments in more detail in laer chapters in the book. The output of the following cell will provide a link at which you can watch your pipeline as it progresses through each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwzYIoEabwRx"
   },
   "outputs": [],
   "source": [
    "pipeline = aiplatform.PipelineJob(display_name=PIPELINE_NAME, template_path='mlops-pipeline.yaml', enable_caching=False)\n",
    "\n",
    "pipeline.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e428aab1826"
   },
   "source": [
    "### Wait for the pipeline to complete\n",
    "The following function will periodically print the status of our pipeline execution. If all goes to plan, you will eventually see a message saying \"PipelineJob run completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b05ed7a9cf3d"
   },
   "outputs": [],
   "source": [
    "pipeline.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the pipeline has completed, you can view the generated image and the prediction outputs in the bucket you specified in GCS.\n",
    "\n",
    "If you used the conventions suggested in this notebook, you will find the generated image and the prediction outputs in your bucket at the following paths:\n",
    "\n",
    "* Generated image: `chapter-18-data/`\n",
    "* Prediction results: `chapter-18-predictions/chapter-18-data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!! You have officially built an end-to-end event-driven solution that combines MLOps & Gen AI on Google Cloud!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1169ce7bd4c8"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them**\n",
    "\n",
    "**If you want to delete the resources, set the `clean_up` paramater to `True`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Vertex AI resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import exceptions as gcp_exceptions\n",
    "if clean_up:  \n",
    "    try:\n",
    "        endpoint_list = aiplatform.Endpoint.list(filter=f'display_name=\"{ENDPOINT_NAME}\"')\n",
    "        if endpoint_list:\n",
    "            endpoint = endpoint_list[0]  # Assuming only one endpoint with that name\n",
    "\n",
    "            # Undeploy all models (if any)\n",
    "            try:\n",
    "                endpoint.undeploy_all()\n",
    "                print(f\"Undeployed all models from endpoint: {ENDPOINT_NAME}\")\n",
    "            except gcp_exceptions.NotFound:\n",
    "                print(f\"No models found to undeploy from endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:  # Catching general errors for better debugging\n",
    "                print(f\"Unexpected error while undeploying models: {e}\")\n",
    "\n",
    "            # Delete endpoint\n",
    "            try:\n",
    "                endpoint.delete()\n",
    "                print(f\"Deleted endpoint: {ENDPOINT_NAME}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "        else:\n",
    "            print(f\"No endpoint found matching: {ENDPOINT_NAME}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Endpoint not found: {ENDPOINT_NAME}\")\n",
    "\n",
    "    # Delete models\n",
    "    try:\n",
    "        model_list = aiplatform.Model.list(filter=f'display_name=\"{MODEL_NAME}\"')\n",
    "        if model_list:\n",
    "            for model in model_list:\n",
    "                print(f\"Deleting model: {model.display_name}\")\n",
    "                model.delete()\n",
    "        else:\n",
    "            print(f\"No models found matching: {MODEL_NAME}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Model not found: {MODEL_NAME}\")\n",
    "\n",
    "    # Delete pipeline\n",
    "    try:\n",
    "        pipeline.delete()\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Pipeline not found: {pipeline.name}\")\n",
    "\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete artifact repository and pubsub topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4a76167aee4"
   },
   "outputs": [],
   "source": [
    "if clean_up:\n",
    "    # Delete the pubsub topic    \n",
    "    ! gcloud pubsub topics delete $TOPIC_NAME\n",
    "    \n",
    "    # Delete the Artifact repository\n",
    "    ! gcloud artifacts repositories delete $TRAIN_REPO_NAME --location=$REGION --quiet\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cloud Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "if clean_up:\n",
    "    # Delete the Cloud Functions\n",
    "    ! gcloud functions delete {GEN_FUNCTION} --quiet\n",
    "    ! gcloud functions delete {PREDICT_FUNCTION} --quiet\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Lineage Metadata\n",
    "\n",
    "# WARNING: THE FOLLOWING CODE WILL DELETE ALL CONTEXTS, EXECUTIONS, AND ARTIFACTS. \n",
    "\n",
    "If you want to delete those resources, set the `delete_metadata` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_metadata = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_metadata: \n",
    "    \n",
    "    # Delete the artifacts\n",
    "    try:\n",
    "        artifacts = aiplatform.Artifact.list()\n",
    "        # To delete specific artifacts, you can filter your Artifact.list() like below:\n",
    "        # artifacts = aiplatform.Artifact.list(filter='schema_title=\"system.Model\"') # Deletes all artifacts with schema title as \"system.Model\"\n",
    "\n",
    "        if not artifacts:\n",
    "            print(\"No Artifacts found in the project and region.\")\n",
    "        else:\n",
    "            for artifact in artifacts:\n",
    "                try:\n",
    "                    artifact.delete()\n",
    "                    print(f\"Deleted Artifact: {artifact.resource_name}\")\n",
    "                except gcp_exceptions.FailedPrecondition as e:\n",
    "                    print(f\"Failed to delete Artifact {artifact.resource_name}: {e}\")\n",
    "                    # Handle specific precondition failures if needed\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error deleting Artifact {artifact.resource_name}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing or deleting Artifacts: {e}\") \n",
    "    \n",
    "    # Delete the contexts\n",
    "    try:\n",
    "        contexts = aiplatform.Context.list()\n",
    "        if not contexts:\n",
    "            print(\"No Contexts found in the project and region.\")\n",
    "        else:\n",
    "            for context in contexts:\n",
    "                try:\n",
    "                    context.delete()\n",
    "                    print(f\"Deleted Context: {context.name}\")\n",
    "                except gcp_exceptions.FailedPrecondition as e:\n",
    "                    print(f\"Failed to delete Context {context.name}: {e}\")\n",
    "                    # Handle specific precondition failures (e.g., Context in use)\n",
    "                except Exception as e:  # Catching general errors for better debugging\n",
    "                    print(f\"Unexpected error while deleting Context {context.name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing or deleting Contexts: {e}\")\n",
    "\n",
    "    # Delete the executions\n",
    "    try:\n",
    "        executions = aiplatform.Execution.list()\n",
    "        if not executions:\n",
    "            print(\"No Executions found in the project and region.\")\n",
    "        else:\n",
    "            for execution in executions:\n",
    "                try:\n",
    "                    execution.delete()\n",
    "                    print(f\"Deleted Execution: {execution.name}\")\n",
    "                except gcp_exceptions.FailedPrecondition as e:\n",
    "                    print(f\"Failed to delete Execution {execution.name}: {e}\")\n",
    "                    # Handle specific precondition failures if needed\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error deleting Execution {execution.name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing or deleting Executions: {e}\")   \n",
    "\n",
    "    # Delete the experiments\n",
    "    try:\n",
    "        # List all experiments \n",
    "        experiments = aiplatform.Experiment.list() \n",
    "    \n",
    "        if not experiments:\n",
    "            print(\"No experiments found in the project and region.\")\n",
    "        else:\n",
    "        # Delete each experiment\n",
    "            for experiment in experiments:\n",
    "                try:\n",
    "                    experiment.delete()\n",
    "                    print(f\"Deleted experiment: {experiment.name}\")\n",
    "                except exceptions.FailedPrecondition as e:\n",
    "                    print(f\"Failed to delete experiment {experiment.name}: {e}\")\n",
    "                    # Handle specific precondition failures if needed (e.g., experiment runs still exist)\n",
    "                except exceptions.NotFound:\n",
    "                    print(f\"Experiment {experiment.name} not found, likely already deleted.\")\n",
    "                except Exception as e:  # Catching general errors for better debugging\n",
    "                    print(f\"Unexpected error while deleting experiment {experiment.name}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing or deleting experiments: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"delete_metadata parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_dataproc_tabular.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
