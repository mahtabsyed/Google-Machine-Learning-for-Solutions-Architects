{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437a25c3-a703-4a0f-8f9f-f7d0aa8496b7",
   "metadata": {},
   "source": [
    "# RAG piece-by-piece on Vertex AI\n",
    "In this notebook, we will build a RAG implementation piece by piece on Vertex AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Document AI Pricing](https://cloud.google.com/document-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eaae40-512c-4411-aba3-b2ebd62d4b3d",
   "metadata": {},
   "source": [
    "## Background / overview\n",
    "\n",
    "There are many different ways to build RAG implementations on Vertex AI. In Chapter 17, we implemented a RAG solution using Vertex AI Search. Remember what we've mentioned numerous times throughout this book: When there are multiple potential ways of implementing a solution, a solution architecture best practice is to always use the most managed solution possible that meets yourn technical and business needs (which may include cost, but that cost should always be considered in terms of \"total cost of ownership\" (TCO), which includes the toil of building and managing solutions ourselves). \n",
    "\n",
    "For that reason, when implementing a RAG solution in Vertex AI, I recommend starting with Vertex AI Search, as we did in Chapter 17. However, if you want to build your own RAG solution from scratch, in which you perform document chunking and embedding explicitly yourself, you can use services such as Google Cloud Document AI and Vertex AI Vector Search (among others)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4294d-26c4-48ad-b837-867557b83887",
   "metadata": {},
   "source": [
    "This notebook shows an example of using Google Cloud Document AI and Vertex AI Vector Search, as well as Google models such as Gemini and textembedding-gecko."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9effd-bcea-4e1a-9b85-8d94d9450a9d",
   "metadata": {},
   "source": [
    "## Solution Architecture\n",
    "\n",
    "The solution architecture is shown in the following diagram:\n",
    "\n",
    "<img src=\"images/RAG-Vertex.png\">\n",
    "\n",
    "The steps in the diagram are described as follows:\n",
    "\n",
    "1. Our documents, which are stored in Google Cloud Storage (GCS), are sent to Google Cloud Document AI for chunking. As the name suggests, the chunking process breaks the documents into “chunks,” which are smaller sections of the document. This is required in order to create standard-sized chunks that serve as inputs to the embedding process. The size of the chunks is configurable in Document AI, and further details on this process are described in the Jupyter Notebook. \n",
    "2. The chunks are then sent to the Google Cloud “textembedding-gecko” LLM to create embeddings for the chunks. The resulting embeddings are stored in GCS, alongside their respective chunks (this step is omitted from the diagram).\n",
    "3. We create a Vertex AI Vector Search index, and the embeddings are ingested from GCS to the Vertex AI Vector Search index (the GCS intermediary step is omitted from the diagram).\n",
    "4. Next, the application/user asks a question that relates to the contents of our documents. The question is sent as a query to textembedding-gecko to be embedded/vectorized.\n",
    "5. The vectorized query is then used as an input in a request to Vertex AI Vector Search, which searches our index to find similar embeddings. Remember that the embeddings represent an element of semantic meaning, so similar embeddings have similar meanings. This is how we can perform a semantic search to find embeddings that are similar to our query.\n",
    "6. Next, we take the embeddings returned from our Vertex AI Vector Search query and find the chunks in GCS that relate to those embeddings (remember that Step 2 in our solution created a stored association of chunks and embeddings).\n",
    "7. Now, it’s finally time to send a prompt to Gemini. The retrieved document chunks from Step 6 serve as context for the prompt. This helps Gemini to respond to our prompt based on the relevant contents from our documents and not just from its pre-trained knowledge.\n",
    "8. Gemini responds to the prompt.\n",
    "\n",
    "Note, between some of the steps depicted in the diagram, Google Cloud Storage is used to store the inputs and outputs of each step, but those intermediary processes are omitted to make the diagram more readable. Also, when we implement this solution here in our Jupyter Notebook, the notebook is the “application/user” that coordinates each of the steps in the overall process.\n",
    "\n",
    "Also, in this case, we are using documents stored in GCS as our source of truth, but we could also use other data, such as data stored in BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b12a3d-1590-414b-80e8-7df07bf402a4",
   "metadata": {},
   "source": [
    "## Document/data citation\n",
    "The citation for the document used in this exercise is as follows:\n",
    "\n",
    "*Hila Zelicha, Jieping Yang, Susanne M Henning, Jianjun Huang, Ru-Po Lee, Gail Thames, Edward H Livingston, David Heber, and Zhaoping Li, 2024. Effect of cinnamon spice on continuously monitored glycemic response in adults with prediabetes: a 4-week randomized controlled crossover trial. DOI:https://doi.org/10.1016/j.ajcnut.2024.01.008*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5d515-7b19-44d4-9efc-fbf782234df5",
   "metadata": {},
   "source": [
    "# Implementation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53862262-f7b8-42f2-b661-7c67de75ef69",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3a8df-cc88-4f78-973a-4d038bc7786a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform google-cloud-storage google-cloud-documentai vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5092e06-64ed-485f-9e24-e1198271b83f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefcdf0-4ef3-4895-8700-347f8cdeceef",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c534-09a1-49e1-a6a5-3fd1b0417081",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6de65-79e8-46ba-a1e8-2959d22f96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "PROJECT_NUMBER_DETAILS = !gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\" \n",
    "PROJECT_NUMBER = PROJECT_NUMBER_DETAILS[0]  # The project number is item 0 in the list returned by the gcloud command \n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Project Number: {PROJECT_NUMBER}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868271aa-3ec1-45d7-be9d-c36aef4ef395",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2de5e-c2ef-40a1-a49e-9614f1107b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1592827-4f0c-442c-beb7-04e18f2bb58e",
   "metadata": {},
   "source": [
    "## Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f167aa-2fd8-425c-9545-f8a3fafec46c",
   "metadata": {},
   "source": [
    "### Create UID for this session\n",
    "\n",
    "This will be used in various variable values throughout this notebook to make the values unique to this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97137f9b-19a3-420f-81cf-defd29b27f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# generate a unique id for this session\n",
    "UID = datetime.now().strftime(\"%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251733c-d577-4fb6-97e0-a668e6422197",
   "metadata": {},
   "source": [
    "## Create Document AI Processor\n",
    "\n",
    "We will create a Document AI Processor to break our input documents into chunks.\n",
    "\n",
    "See documentation [here](https://cloud.google.com/document-ai/docs/overview#dai-processors) and [here](https://cloud.google.com/document-ai/docs/layout-parse-chunk) for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2c7da-20a0-42c9-bc5f-75c958a9f613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "\n",
    "# Document AI Variables\n",
    "location = 'us'  # # Different from REGION; the format is 'us' or 'eu'\n",
    "processor_display_name = f\"RAG-Chunking-Processor-{UID}\"\n",
    "processor_type = 'LAYOUT_PARSER_PROCESSOR'\n",
    "\n",
    "# Create Document AI client\n",
    "opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "# Location path\n",
    "parent = client.common_location_path(PROJECT_NUMBER, location)\n",
    "\n",
    "# Create the processor \n",
    "processor = client.create_processor(\n",
    "    parent=parent,\n",
    "    processor=documentai.Processor(\n",
    "        display_name=processor_display_name, type_=processor_type\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Print the processor information\n",
    "print(f\"Processor Name: {processor.name}\")\n",
    "print(f\"Processor Display Name: {processor.display_name}\")\n",
    "print(f\"Processor Type: {processor.type_}\")\n",
    "print(f\"Processor State: {processor.state}\")\n",
    "\n",
    "processor_name = processor.name  # Get the processor name for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e93cc-7df0-437e-a4e5-3e8ac0bd40ca",
   "metadata": {},
   "source": [
    "## Process the document to create the chunks\n",
    "\n",
    "This is the point at which we perform document chunking.\n",
    "\n",
    "The following code will use our Document AI processor to break our document into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833732e-54e1-4ae5-95d3-75eaa13948d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "file_path = \"data/Cinnamon.pdf\"\n",
    "mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_name: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    ") -> None:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    name = processor_name\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load binary data\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # For more information: https://cloud.google.com/document-ai/docs/reference/rest/v1/ProcessOptions\n",
    "\n",
    "    # Define the main configuration dictionary\n",
    "    process_options = documentai.ProcessOptions(\n",
    "        layout_config=documentai.ProcessOptions.LayoutConfig(\n",
    "            chunking_config=documentai.ProcessOptions.LayoutConfig.ChunkingConfig(\n",
    "                chunk_size=200,  \n",
    "                include_ancestor_headings=True\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name,\n",
    "        raw_document=raw_document,\n",
    "        process_options=process_options,\n",
    "    )\n",
    "    \n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # For a full list of `Document` object attributes, reference this page:\n",
    "    # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\n",
    "    document = result.document\n",
    "    return document\n",
    "\n",
    "document_object = process_document_sample(PROJECT_NUMBER, location, processor_name, file_path, mime_type)\n",
    "\n",
    "doc_layout = document_object.document_layout\n",
    "chunked_doc = document_object.chunked_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d58d5b-ccd1-4c44-ba87-b59a77710273",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Notes on tokens and chunks\n",
    "\n",
    "#### Tokens\n",
    "\n",
    "LLMs generally work with `tokens` rather than words. When dealing with text, a token often represents subsections of words, and tokenization can be done in different ways, such as breaking text up by characters, or using subword-based tokenization (e.g., the word \"unbelievable\" could be split into subwords such as \"un\", \"believe\", \"able\").\n",
    "\n",
    "The exact size and definition of a token can vary based on different tokenization methods, models, languages, and other factors, but a general rule of thumb for English text using subword tokenizers is around 4 characters per token on average.\n",
    "\n",
    "#### Chunks\n",
    "\n",
    "When creating embeddings, we usually break a document into chunks and then create embeddings of those chunks. Again, this can be done in different ways, using different tools. In this notebook, we're using Google Cloud Document AI to break our documents into chunks.\n",
    "\n",
    "For this purpose, one of the parameters we need to specify for our Document AI Processor is the `chunkSize` to use, which is measured (in this case) by number of tokens per chunk. You may need to experiment with the value for this parameter to find the chunk size that works best for your use case (e.g., based on the length and structure of the document sections). You generally want your chunks to capture some level of semantic granularity, but there are trade-offs in terms of this granularity. For example, smaller chunks can capture more granular sematic context, and can provide more precise search results, but can be less efficient (computationally) to process. You also need to ensure chunks sizes are within the input length limits of the embedding model you're using in order to avoid possible truncation.\n",
    "\n",
    "A good practice is to start with a moderate chunk size and adjust it based on how well it fits your needs.\n",
    "\n",
    "Fortunately, Document AI can automatically handle some chunking based on layout, even without a set chunkSize, so that can be helpful if you don't know what chunk size to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced125c-1808-4c64-bbbf-039922813d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Review document layout and chunks\n",
    "\n",
    "#### Print sample of document layout\n",
    "\n",
    "The following code will print the first couple of blocks from our document layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e4d89-019e-40fe-9729-51e7fdd806bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(doc_layout.blocks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2f2ce-cda7-4da7-ac6d-07f2a9c26199",
   "metadata": {},
   "source": [
    "### Review the text chunks\n",
    "\n",
    "The following code will consolidate our document chunks into a list that we can use from here onwards.\n",
    "\n",
    "We will also print the first 5 chunks to see their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507aa5d7-1706-4260-8a1e-1cd6e2270c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_chunks = [chunk.content for chunk in chunked_doc.chunks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b2132-0558-471e-a0b2-42eca4b4671a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text_chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b25ef48-2930-4c41-bfbe-c38dd8c4a266",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "Now that we have broken our document down into chunks, the next steps is to get embeddings for our chunks.\n",
    "\n",
    "We will use the `textembedding-gecko` model to create our embeddings (see [documentation here](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b71f8-9921-4bfe-b242-a35adedc4bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "def embed_text(\n",
    "    texts: List[str] = text_chunks,  # Use your extracted chunks\n",
    "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
    "    model_name: str = \"textembedding-gecko@003\",\n",
    ") -> List[List[float]]:\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "    embeddings = model.get_embeddings(inputs)\n",
    "    return [embedding.values for embedding in embeddings]\n",
    "\n",
    "embeddings = embed_text(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c56198-dfe7-4f3b-96c6-366296feb74a",
   "metadata": {},
   "source": [
    "### Save embeddings\n",
    "\n",
    "Next, we will save our embeddings in a local file.\n",
    "\n",
    "The code in the following cell will also save the text chunks with their associated embeddings, which we can use for retrieval later.\n",
    "\n",
    "This file will be used as input when creating a Vertex AI Vector Search index in subsequent sections of this notebook.\n",
    "\n",
    "See the documentation [here](https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure) for requirements regarding the input format structure for Vertex AI Vector Search indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bbbc9-825a-4298-a737-9811c9290467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "filename = f\"embeddings-{UID}.json\"\n",
    "\n",
    "def create_embeddings_jsonl(text_chunks, embeddings, filename):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        for idx, (text, embedding) in enumerate(zip(text_chunks, embeddings)):\n",
    "            data = {\n",
    "                \"id\": idx,\n",
    "                \"text\": text,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            json.dump(data, outfile, separators=(',', ':'))\n",
    "            outfile.write('\\n')\n",
    "\n",
    "create_embeddings_jsonl(text_chunks, embeddings, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df731b-4d64-4cf3-a0cb-ec7de045e3cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload file to GCS\n",
    "\n",
    "Next, we will upload our file to Google Cloud Storage (GCS). This is required for us to ingest our embeddings into Vertex AI Vector Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e917fb2-673e-43e1-a0d7-cb0b5e4f7ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_path = f\"gs://{BUCKET}/chapter-18-embeddings-data-{UID}/batch_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694caf2-d178-41ee-b905-abb476b6617f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp {filename} {embeddings_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01201d0-ed25-4e8b-a05a-3589a21c5baf",
   "metadata": {},
   "source": [
    "## Create Vertex AI Vector Search Index\n",
    "\n",
    "We will store our embeddings in Vertex AI Vector Search. To do that we need to create an index and specify the GCS location of our embeddings file to be ingested.\n",
    "\n",
    "See the documentation [here](https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index) and [here](https://cloud.google.com/vertex-ai/docs/vector-search/configuring-indexes) for more details on creating Index and the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c72beb-f0ab-4fd1-adfa-7dd19fa62130",
   "metadata": {},
   "source": [
    "### First, get the embedding dimensionality\n",
    "\n",
    "When creating an index in Vertex AI Vector Search, we need to know the dimensionality of our embeddings. We can get that with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371d479-3299-470a-81a8-7a0f871baf02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_embedding = embeddings[0] # Assumes all our embeddings have same dimensionality, so we can just use the first one.\n",
    "embeddings_dimensionality = len(first_embedding) \n",
    "print(f\"Embeddings dimensionality: {embeddings_dimensionality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63405a-4f47-4377-9bcd-30eacee325ab",
   "metadata": {},
   "source": [
    "### Create the index\n",
    "\n",
    "This process can take a long time, depending on the amount of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb6fe2-d51e-4f9e-8fdc-79e8f7c8106b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def vector_search_create_index(\n",
    "    project: str, location: str, display_name: str, gcs_uri: Optional[str] = None\n",
    ") -> None:\n",
    "    # Initialize the Vertex AI client\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=BUCKET)\n",
    "\n",
    "    # Create Index\n",
    "    index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "        display_name=display_name,\n",
    "        contents_delta_uri=f\"{embeddings_path}\",\n",
    "        description=\"RAG Index\",\n",
    "        dimensions=embeddings_dimensionality,\n",
    "        approximate_neighbors_count=50,\n",
    "        leaf_node_embedding_count=500,\n",
    "        leaf_nodes_to_search_percent=7,\n",
    "        index_update_method=\"batch_update\",  \n",
    "        distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    )\n",
    "    return(index)\n",
    "\n",
    "location = \"us-central1\"\n",
    "display_name = f\"RAG-index-{UID}\"\n",
    "vvs_index = vector_search_create_index(PROJECT_ID, REGION, display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7154f4-67c5-49d0-81cc-1de872d4fffa",
   "metadata": {},
   "source": [
    "## Create Endpoint and Deploy Index\n",
    "\n",
    "To use our index, we need to deploy it to an endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d97bf3-5afa-4a3e-8c61-f1285080fce4",
   "metadata": {},
   "source": [
    "### First, create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6f6a4-c76e-4733-8d3f-46c8efedf2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create `IndexEndpoint`\n",
    "vvs_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name = f\"index-endpoint-{UID}\",\n",
    "    public_endpoint_enabled = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382063d4-fb09-4ecd-89a1-cd79be8dea2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the index to the endpoint\n",
    "\n",
    "**This step can take a long time.**\n",
    "\n",
    "To check the status in the Google Cloud console, navigate to Vertex AI -> Vector Search, and click on the name of your index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0c85-2b46-4cd4-894b-553f0e0c64b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an ID for the index\n",
    "DEPLOYED_INDEX_ID = f\"deployed_index_{UID}\"\n",
    "\n",
    "vvs_index_endpoint.deploy_index(\n",
    "    index = vvs_index, deployed_index_id = DEPLOYED_INDEX_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40a918-a227-4407-a60f-1011b1da94a8",
   "metadata": {},
   "source": [
    "## Perform a vector-based similarity search\n",
    "\n",
    "Now that we have created our index, we can use it to perform a vector-based similarity search to find the nearest neighbors to our query.\n",
    "\n",
    "For this work, we need to first create an embedding of our query using the same model that was used to create embeddings for our text chunks earlier (in our case, we used `textembedding-gecko@003`). This is done using the `embed_query` function.\n",
    "\n",
    "Let's use one of our questions from Chapter 16 (feel free to replace the `query_text` string with other questions related to the document contents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401aa939-131f-403f-b3ae-92ff0a2894f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embedding for input query\n",
    "def embed_query(text: str, task: str = \"RETRIEVAL_DOCUMENT\", model_name: str = \"textembedding-gecko@003\"):\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "    input = TextEmbeddingInput(text, task)  # Create a single input object\n",
    "    embeddings = model.get_embeddings([input])  # Pass input as a list\n",
    "    return embeddings[0].values  # Return the embedding as a list of floats\n",
    "\n",
    "query_text = \"How might cinnamon supplementation interact with other dietary or lifestyle interventions for prediabetes management?\"\n",
    "query_embedding = embed_query(query_text)\n",
    "\n",
    "# Find the nearest neighbors\n",
    "response = vvs_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=[query_embedding],\n",
    "    num_neighbors=3 # You can change this value to return more or fewer neighbors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88046d3-37ba-48f1-87e5-8aa846244506",
   "metadata": {},
   "source": [
    "### Print details of all of the nearest neighbors returned\n",
    "\n",
    "In the response, we can see the details of the nearest neighbors returned, including their IDs and distances relative to our query.\n",
    "\n",
    "Usually, smaller distance means that the returned neighbors are more similar to our query (e.g., Euclidean distance). However, we are using the `DOT_PRODUCT_DISTANCE`, which means that, counter-intuitively, the higher the value of the `distance` metric, the closer and more similar the returned neighbor is to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51574f-686f-4f48-b441-a5ca5687179a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13456e9b-edf1-4297-af25-08ae0e635f77",
   "metadata": {},
   "source": [
    "### Print details of the nearest neighbor returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581ad59-8e85-4f01-974b-e447210d931e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d8913-091f-44b3-8eea-499f10466f57",
   "metadata": {},
   "source": [
    "### Inspect the nearest neighbor\n",
    "\n",
    "The following code will display the nearest neighbor embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098138df-cffa-4cb4-bb71-7f4030d6b92b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neighbor_id = response[0][0].id\n",
    "neighbor_embedding = vvs_index_endpoint.read_index_datapoints(deployed_index_id=DEPLOYED_INDEX_ID, ids= [neighbor_id],)\n",
    "print(neighbor_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833c0a8-25d8-4271-9b9f-b2cf2662bf97",
   "metadata": {},
   "source": [
    "## Retrieve text chunks represented by the nearest neighbor embeddings\n",
    "\n",
    "The embeddings are used for performing similarity/neighbor search in the vector space.\n",
    "\n",
    "If we want to implement a RAG use case, however, the embeddings by themselves are not meaningful to include in the context for interacting with a generative model. \n",
    "\n",
    "For that reason, we need to retrieve the text chunks that are associated with the nearest neighbor embeddings.\n",
    "\n",
    "The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0e06a-72d7-4839-a24a-5154b875e6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract IDs from the response\n",
    "neighbor_ids = [neighbor.id for neighbor in response[0]]  # Access the first (and likely only) list in the response\n",
    "\n",
    "# Fetch the text chunks corresponding to given IDs from the JSONL file\n",
    "def fetch_text_chunks(ids, filename=filename):\n",
    "    texts = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if str(data['id']) in ids:  # Ensure the id from JSON is treated as a string for matching\n",
    "                texts[str(data['id'])] = data['text']\n",
    "    return [texts[id] for id in ids if id in texts]\n",
    "\n",
    "# Fetch the corresponding texts using the IDs extracted\n",
    "neighbor_texts = fetch_text_chunks([str(id) for id in neighbor_ids], filename=filename)  \n",
    "\n",
    "# Now we have the embeddings and their corresponding text chunks\n",
    "print(neighbor_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d7659-e38a-4deb-bc38-bbe6483366b7",
   "metadata": {},
   "source": [
    "## Use retrieved content with generative AI model\n",
    "\n",
    "To implement a RAG use case, we can include the retrieved content as context when sending a prompt to a generative AI model.\n",
    "\n",
    "The following code will send a prompt to Gemini, and will include our retrieved text chunks as context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260547b7-022a-4dc6-a493-f2aa9df0b151",
   "metadata": {},
   "source": [
    "### First, send a simple prompt to Gemini, without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd7763-b055-4d8c-a67c-6ad491965a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "model = GenerativeModel(model_name=\"gemini-1.0-pro-002\")\n",
    "chat = model.start_chat()\n",
    "\n",
    "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
    "    text_response = []\n",
    "    responses = chat.send_message(prompt, stream=True)\n",
    "    for chunk in responses:\n",
    "        text_response.append(chunk.text)\n",
    "    return \"\".join(text_response)\n",
    "\n",
    "# Using our query text from earlier\n",
    "prompt = query_text\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4110658-172a-4103-a78f-1a84809adf45",
   "metadata": {},
   "source": [
    "### Provide context\n",
    "\n",
    "In the next prompt, we provide our retrieved text chunks as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ff0d1-cd4c-41df-8ba0-b32c343cadf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"{query_text} in the context of:{neighbor_texts}\"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9818e2-21f4-48fc-8f74-bbea30216ecf",
   "metadata": {},
   "source": [
    "### Try with precise answers only from the source document\n",
    "\n",
    "In this case, we instruct the model to answer only from the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb5d96-22a1-42b0-b2d0-a273e16a593c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"{query_text} Answer only from the following context:{neighbor_texts}\"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabaae8-0dfc-4fd5-97a4-959944a7fc72",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb76c1-7a07-4d89-ada7-21245c7be5ba",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff44cec-cabd-497f-9432-0cc63b74d8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285785d-5014-4ef1-9358-0a1949efe3f9",
   "metadata": {},
   "source": [
    "## Delete the Document AI Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c713fc-c3b5-4e6b-a85e-99d4dd6866df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import exceptions as gcp_exceptions\n",
    "if clean_up:\n",
    "    try:\n",
    "        client.delete_processor(name=processor_name)\n",
    "        print(f\"Deleted Processor: {processor_name}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Processor not found: {processor_name}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e1ff3-f58c-4519-ac0a-9c7f732d1c5b",
   "metadata": {},
   "source": [
    "## Delete the Vertex Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51f641-e73f-4546-ac1c-00e541cb8407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:\n",
    "    try:\n",
    "        vvs_index.delete()  # Set force=True to bypass the check for deployed indexes\n",
    "        print(f\"Deleted Matching Engine Index: {index.resource_name}\")\n",
    "    except gcp_exceptions.NotFound:\n",
    "        print(f\"Index not found: {index.resource_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting index: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407cabc-e1e8-4b39-aa92-818a10b283c3",
   "metadata": {},
   "source": [
    "## Delete the Vertex Vector Search Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d29ea-3259-4118-86bb-e8e1c195f9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:\n",
    "    try:\n",
    "        # Undeploy the deployed index\n",
    "        try:\n",
    "            vvs_index_endpoint.undeploy_index(deployed_index_id=DEPLOYED_INDEX_ID)\n",
    "            print(f\"Undeployed index '{DEPLOYED_INDEX_ID}' from endpoint '{vvs_index_endpoint.name}'.\")\n",
    "        except exceptions.NotFound:\n",
    "            print(f\"Deployed index '{DEPLOYED_INDEX_ID}' not found in endpoint '{vvs_index_endpoint.name}'.\")\n",
    "        \n",
    "        # Delete the index endpoint\n",
    "        vvs_index_endpoint.delete()\n",
    "        print(f\"Deleted index endpoint: {vvs_index_endpoint.name}\")\n",
    "\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"Index endpoint not found: {vvs_index_endpoint.name}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2f8a5-4748-4be8-bc1e-f5007f00d471",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2e150-d910-415e-add3-b8274ee766da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca201-3eaa-4e13-b92a-6b8902df7ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
