{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477267b4-755f-42da-9e47-ebd4f8dd7dc7",
   "metadata": {},
   "source": [
    "# Using pandas with BigQuery via pandas-gbq\n",
    "\n",
    "In this notebook, we explore how to use pandas with BigQuery via the pandas_gbq library.\n",
    "\n",
    "It's an open-source library that is maintained by PyData and volunteer contributors and has been around for quite some time (since 2017), so it has become quite broadly used in the industry.\n",
    "\n",
    "Essentially, itâ€™s a thin wrapper around the BigQuery client library (google-cloud-bigquery) that provides a simple interface for running SQL queries and uploading pandas dataframes to BigQuery. The results from these queries are parsed into a pandas.DataFrame object in which the shape and data types are derived from the source table.\n",
    "\n",
    "Let's dive in and start using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [BigQuery Pricing](https://cloud.google.com/bigquery/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816bb5a-7266-49ea-90af-42c6cdf3257b",
   "metadata": {},
   "source": [
    "## Install and import libraries\n",
    "\n",
    "We start by installing and importing the required libraries (pandas, pandas_gbq, and numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8313-4ed8-4962-b0b2-fd990d113f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install --quiet pandas pandas-gbq numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dd596-48fa-4d57-af4a-6622317a28eb",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, restart the notebook kernel so it can find the packages.\n",
    "Click OK when prompted after running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73f6a1-5b17-403a-b36f-9c809ba4bec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs\n",
    "import os\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prevent further cells from executing until the kernel restarts\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aff328-bf53-4fb3-b838-3106f4fbd5b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (Wait for the kernel to restart before continuing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad4c67-baee-45c7-8f23-d82b202c79f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c282ab2-7818-4900-8ca4-c7e8802da040",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "\n",
    "Next, we define the constants to contain our project ID and the dataset ID at which we will save our data in BigQuery later.\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35767946-dbbd-44de-885a-2a9946ffda43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "UPDATED_DATASET_ID = \"new_york_taxi_trips\"\n",
    "TABLE = \"transformed_taxi_data_pandas_gbq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f7f4ef-ddb5-4eeb-9881-3ca0c990d394",
   "metadata": {},
   "source": [
    "## Define and run the query to load our data\n",
    "\n",
    "In the next cell, we will run a simple SQL query to read in some data from the `New York Taxi Trips` BigQuery Public Dataset into a dataframe that we can then use in the remaining steps in this notebook. We will limit the number of records to 1000 because pandas_gbq will download the results of the query to our local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f2063-3147-4cde-92ca-3bab11c838c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUERY = \"SELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2020` LIMIT 1000\"\n",
    "\n",
    "# Use pandas-gbq to load the data into a DataFrame\n",
    "df = pandas_gbq.read_gbq(QUERY, project_id=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddb3b9-cac0-40da-b43a-92f7bccce394",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Now that we've read the data into a dataframe, we can begin to explore our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5018b-7a00-4323-b830-c5dfc8006e82",
   "metadata": {},
   "source": [
    "### Preview the data\n",
    "\n",
    "Let's take a look at some of the values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91678265-bc7d-4ec3-a13c-138ae57fd3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cb17b-d796-4ae7-a19c-626db5b43197",
   "metadata": {},
   "source": [
    "### Explore the data types \n",
    "\n",
    "We can use the dtypes property to explore the data types in the fields of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dff54-d209-4f38-83b3-54cbc4f54228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa76219-f20d-4330-b523-7cde40f3402c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary statistics\n",
    "\n",
    "We can use the describe() function to display some summary statistics about the fields in our dataset. This can help us to understand the scale of features in our dataset, by displaying statistics such as `count`, `min`, `max`, `mean`, and the standard deviation (`std`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357ac70-81a2-410f-9d32-c3f07ddbb15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cf78f-b6bc-4b8d-95ce-e8957cce5e0f",
   "metadata": {},
   "source": [
    "### Explore missing values\n",
    "\n",
    "Missing values can cause problems for many machine learning algorithms, so it's often important for data scientists to be aware of any missing values that exist in the dataset, and to address them accordingly. The code in the next cell will tell us how many missing values exist for each feature in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57668b1-de2a-444b-be29-2e3f2c829f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea6212-ae74-4164-a67c-c2228e7ff8bf",
   "metadata": {},
   "source": [
    "### Value counts\n",
    "\n",
    "It's also often important to understand how many unique values each feature contains. This is referred to as the `cardinality` of a feature, where low cardinality features have a small number of unique values (e.g., binary features that are either `yes` or `no`), and high cardinality have a large number of unique values (e.g., product IDs).\n",
    "\n",
    "Feature cardinality can be important to understand for tasks such as feature encoding and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2c08a-4cc2-4c64-b347-e7c4a14d813e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad7278-c743-4a74-8444-66f33eb7f42f",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "After exploring our data, we can perform any feature engineering that we believe could be important for helping our models to learn specific patterns in our dataset.\n",
    "\n",
    "For example, we can engineer a new feature named `fare_per_mile` by diving the `fare_amount` feature by the `trip_distance` feature, and this new feature may be more useful if we want to build a model that estimates the fare for a given trip distance.\n",
    "\n",
    "To avoid errors such as type mismatches during our division operation, we will convert all types to float.\n",
    "To avoid division by zero, we replace all instances of zero in `trip_distance` with `numpy.finfo.eps` (epsilon), which is a tiny positive number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd2292-cb68-4138-b876-8daa9b0b5e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['fare_amount'] = df['fare_amount'].astype(float)\n",
    "df['trip_distance'] = df['trip_distance'].astype(float)\n",
    "df['fare_per_mile'] = df['fare_amount'] / df['trip_distance'].replace(0, np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6b1d3-43e2-4d20-ace7-ff37a7c81ed1",
   "metadata": {},
   "source": [
    "## Writing data to BigQuery\n",
    "\n",
    "After performing our feature engineering steps, we can write our updated data back to BigQuery for long term storage, reference, and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84583705-103e-437e-ac39-210c4abe8450",
   "metadata": {},
   "source": [
    "### Convert decimal types to float\n",
    "\n",
    "The following is a step that only appears to be necessary when using `pandas_gbq` to write this specific dataset to BigQuery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c245e9-c38b-4fc3-be58-7b8c3837c880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert all columns of type 'decimal.Decimal' to 'float'\n",
    "for column in df.columns:\n",
    "    if pd.api.types.is_object_dtype(df[column]):\n",
    "        try:\n",
    "            df[column] = df[column].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            pass  # or handle non-convertible columns as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480cd67-025b-4d12-8e11-78fe15962556",
   "metadata": {},
   "source": [
    "### Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae49a80-4304-4512-bacb-789c5caf3dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_gbq(f\"{PROJECT_ID}.{UPDATED_DATASET_ID}.{TABLE}\") \n",
    "print(f\"Created table: {PROJECT_ID}.{UPDATED_DATASET_ID}.{TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8798f1-2ad2-4399-933d-31c2ec49417c",
   "metadata": {},
   "source": [
    "**After that operation completes, you can view the dataset in the [BigQuery console](https://console.cloud.google.com/bigquery)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c612f",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d74015",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff8d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf7da1-b4de-494d-9130-b62d53041a18",
   "metadata": {},
   "source": [
    "## Delete BigQuery resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc0c71-2676-4dc1-8e4f-c8262193f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:  \n",
    "    try:\n",
    "        ! bq rm -r -f -d $PROJECT_ID:$UPDATED_DATASET_ID\n",
    "        print(f\"Deleted dataset {UPDATED_DATASET_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b954f-9cd1-4cc9-8066-3951b75f7cd0",
   "metadata": {},
   "source": [
    "**You can also verify or delete the dataset in the [BigQuery console](https://console.cloud.google.com/bigquery)**"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
