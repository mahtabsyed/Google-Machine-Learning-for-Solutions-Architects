{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60c0fee-3e4f-46da-bd7d-a7e82098c8e4",
   "metadata": {},
   "source": [
    "# Using Serverless Spark with Google Cloud Vertex AI\n",
    "This notebook provides example steps for using Serverless Spark with Google Cloud Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Introduction and setup\n",
    "\n",
    "In this notebook, we will build a pipeline that will perform the following steps:\n",
    "1. Custom data processing such as feature scaling, one-hot encoding, and feature engineering, in a Google Cloud Serverless Spark environment.\n",
    "2. Implement a custom training job in Vertex AI to train a custom model. In this case, our model uses the [Titanic dataset from Kaggle](https://www.kaggle.com/competitions/titanic/data) to predict the likelihood of survival of each passenger based on their associated features in the dataset.\n",
    "3. Upload the trained model to Vertex AI Model Registry.\n",
    "4. Deploy the trained model to a Vertex AI endpoint for online inference.\n",
    "\n",
    "In this initial section, we set up all of the baseline requirements to run our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95821d-7f47-44e4-9959-b45d38570967",
   "metadata": {},
   "source": [
    "**Attention:** The code in this notebook creates Google Cloud resources that can incur costs.\n",
    "\n",
    "Refer to the Google Cloud pricing documentation for details.\n",
    "\n",
    "For example:\n",
    "\n",
    "* [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Google Cloud Storage Pricing](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc pricing](https://cloud.google.com/dataproc/pricing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install required packages\n",
    "\n",
    "We will use the following libraries in this notebook:\n",
    "\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Kubeflow Pipelines (KFP)](https://www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview/)\n",
    "* [Google Cloud Pipeline Components (GCPC)](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --quiet --user --upgrade google-cloud-aiplatform kfp google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4350d",
   "metadata": {},
   "source": [
    "*The pip installation commands sometimes report various errors. Those errors usually do not affect the activities in this notebook, and you can ignore them.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Kubeflow Pipelines (KFP)\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, Input, Output, Artifact\n",
    "\n",
    "# Google Cloud Pipeline Components (GCPC)\n",
    "from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants\n",
    "In this section, we define all of the constants that will be referenced throughout the rest of the notebook.\n",
    "\n",
    "**REPLACE THE PROJECT_ID, REGION, AND BUCKET DETAILS WITH YOUR DETAILS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQOV9ssPWbMB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core constants\n",
    "BUCKET_URI = f\"gs://{BUCKET}\"\n",
    "TRAINER_DIR = \"pyspark-titanic-training\" # Local parent directory for our pipeline resources\n",
    "PROCESSING_DIR = \"pyspark-titanic-preprocessing\" # Local directory for PySpark data processing resources\n",
    "DATAPROC_RUNTIME_VERSION = \"2.1\" # (See https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions)\n",
    "\n",
    "# Pipeline constants\n",
    "PIPELINE_NAME = \"pyspark-titanic-pipeline\" # Name of our pipeline\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\" # (See: https://www.kubeflow.org/docs/components/pipelines/v1/overview/pipeline-root/)\n",
    "SUBNETWORK = \"default\" # Our VPC subnet name\n",
    "SUBNETWORK_URI = f\"projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{SUBNETWORK}\" # Our VPC subnet resource identifier\n",
    "MODEL_NAME = \"pyspark-titanic-model\" # Name of our model\n",
    "EXPERIMENT_NAME = \"aiml-sa-pyspark-experiment\" # Vertex AI \"Experiment\" name for metadata tracking\n",
    "\n",
    "# Preprocessing constants\n",
    "SOURCE_DATASET = f\"{BUCKET_URI}/data/unprocessed/titanic/train.csv\" # Our raw source dataset\n",
    "PREPROCESSING_PYTHON_FILE_URI = f\"{BUCKET_URI}/code/mlops/preprocessing.py\" # GCS location of our PySpark script\n",
    "PROCESSED_DATA_URI =f\"{BUCKET_URI}/data/processed/mlops-titanic\" # Location to store the output of our data preprocessing step\n",
    "# Arguments to pass to our preprocessing script:\n",
    "PREPROCESSING_ARGS = [\n",
    "    \"--source_dataset\",\n",
    "    SOURCE_DATASET,\n",
    "    \"--processed_data_path\",\n",
    "    PROCESSED_DATA_URI,\n",
    "]\n",
    "\n",
    "# Training constants\n",
    "TRAINING_PYTHON_FILE_URI = f\"{BUCKET_URI}/code/additional-use-cases-chapter/pyspark-ml.py\" # GCS location of our PySpark script\n",
    "MODEL_URI = f\"{BUCKET_URI}/models/additional-use-cases-chapter/mlops\" # Where to store our trained model\n",
    "\n",
    "# Arguments to pass to our training job\n",
    "TRAINING_ARGS=[\n",
    "        \"--processed_data_path\",\n",
    "        PROCESSED_DATA_URI,\n",
    "        \"--model_path\",\n",
    "        MODEL_URI,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local directories\n",
    "We will use the following local directories during the activities in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a source directory to save the code\n",
    "!mkdir -p $TRAINER_DIR\n",
    "!mkdir -p $PROCESSING_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source dataset \n",
    "Upload our source dataset to GCS. Our data preprocessing step in our pipeline will ingest this data from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ./data/train.csv $SOURCE_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set  project ID for  gcloud\n",
    "The following command sets our project ID for using gcloud commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vertex AI SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Private Google Access for Dataproc \n",
    "Our Serverless Spark data preprocessing job in our pipeline will run in Dataproc, which is (as Google defines) Google's \"fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.\"\n",
    "We're going to configure something called \"Private Google Access\", which allows us to interact with Google services without sending requests over the public Internet.\n",
    "\n",
    "You can learn more about Dataproc [here](https://cloud.google.com/dataproc?hl=en), and learn more about Private Google Access [here](https://cloud.google.com/vpc/docs/private-google-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets update $SUBNETWORK --region=$REGION --enable-private-ip-google-access\n",
    "!gcloud compute networks subnets describe $SUBNETWORK --region=$REGION --format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom PySpark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac5229530e16"
   },
   "source": [
    "The following code will create a file that contains the code for our custom PySpark data preprocessing job. \n",
    "\n",
    "The code initiates a Spark session, loads our raw source dataset, and then performs the following processing steps (we performed many of these steps using pandas in our feature engineering chapter earlier in this book, but in this case we will implement the steps using PySpark in Google Cloud Serverless Spark):\n",
    "\n",
    "1. Removes rows from the dataset where the target variable (\"Survived\") is missing values.\n",
    "2. Drops columns that are unlikely to affect the likelihod of surviving, such as 'PassengerId', 'Name', 'Ticket', and 'Cabin'.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS.\n",
    "\n",
    "**Note:** we could create a custom container in which to run our PySpark code on Dataproc Serverless if we had very specific dependencies that needed to be installed. However, Dataproc Serverless also provides [default runtimes](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions) that we can use, and these are fine for our needs in this activity, so all we need to do is define our code and put it into GCS so that it can be referenced by the `DataprocPySparkBatchOp` component in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $PROCESSING_DIR/preprocessing.py\n",
    "\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "\n",
    "# Setting up the argument parser\n",
    "parser = argparse.ArgumentParser(description='Data Preprocessing Script')\n",
    "parser.add_argument('--source_dataset', type=str, help='Path to the source dataset')\n",
    "parser.add_argument('--processed_data_path', type=str, help='Path to save the output data')\n",
    "\n",
    "# Parsing the arguments\n",
    "args = parser.parse_args()\n",
    "source_dataset = args.source_dataset\n",
    "processed_data_path = args.processed_data_path\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "titanic = spark.read.csv(args.source_dataset, header=True, inferSchema=True)\n",
    "\n",
    "# Remove rows where 'Survived' is missing\n",
    "titanic = titanic.filter(titanic.Survived.isNotNull())\n",
    "\n",
    "# Drop irrelevant columns\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket', 'Cabin')\n",
    "\n",
    "# Fill missing values\n",
    "def calculate_median(column_name):\n",
    "    return titanic.filter(col(column_name).isNotNull()).approxQuantile(column_name, [0.5], 0)[0]\n",
    "\n",
    "median_age = calculate_median('Age')  # Median age\n",
    "median_fare = calculate_median('Fare')  # Median fare\n",
    "\n",
    "titanic = titanic.fillna({\n",
    "    'Pclass': -1,\n",
    "    'Sex': 'Unknown',\n",
    "    'Age': median_age,\n",
    "    'SibSp': -1,\n",
    "    'Parch': -1,\n",
    "    'Fare': median_fare,\n",
    "    'Embarked': 'Unknown'\n",
    "})\n",
    "\n",
    "# Feature Engineering\n",
    "titanic = titanic.withColumn('FamilySize', col('SibSp') + col('Parch') + 1)\n",
    "titanic = titanic.withColumn('IsAlone', when(col('FamilySize') == 1, 1).otherwise(0))\n",
    "\n",
    "# Define categorical features \n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'IsAlone']\n",
    "\n",
    "# Define numerical features \n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "stages = []\n",
    "for col_name in categorical_features:\n",
    "    string_indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{col_name}_Index\"], outputCols=[f\"{col_name}_Vec\"])\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "# Scaling numerical features \n",
    "for col_name in numerical_features:\n",
    "    assembler = VectorAssembler(inputCols=[col_name], outputCol=f\"vec_{col_name}\")\n",
    "    scaler = StandardScaler(inputCol=f\"vec_{col_name}\", outputCol=f\"scaled_{col_name}\", withStd=True, withMean=False)\n",
    "    stages += [assembler, scaler]\n",
    "\n",
    "# Create a pipeline and transform the data\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_model = pipeline.fit(titanic)\n",
    "titanic = pipeline_model.transform(titanic)\n",
    "\n",
    "# Drop intermediate columns created during scaling and one-hot encoding\n",
    "titanic = titanic.drop('vec_Age', 'vec_Fare', 'vec_FamilySize', 'vec_SibSp', 'vec_Parch', 'Pclass_Index', 'Sex_Index', 'Embarked_Index', 'IsAlone_Index')\n",
    "\n",
    "# Drop original categorical columns (no longer needed after one-hot encoding)\n",
    "titanic = titanic.drop(*categorical_features)\n",
    "\n",
    "# Drop original numeric columns (no longer needed after scaling)\n",
    "titanic = titanic.drop(*numerical_features)\n",
    "\n",
    "vector_columns = [\"Pclass_Vec\", \"Sex_Vec\", \"Embarked_Vec\", \"IsAlone_Vec\", \"scaled_Age\", \"scaled_Fare\", \"scaled_FamilySize\", \"scaled_SibSp\", \"scaled_Parch\"]\n",
    "\n",
    "def to_dense(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "to_dense_udf = udf(to_dense, ArrayType(FloatType()))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    titanic = titanic.withColumn(vector_col, to_dense_udf(col(vector_col)))\n",
    "\n",
    "for vector_col in vector_columns:\n",
    "    num_features = len(titanic.select(vector_col).first()[0])  # Getting the size of the vector\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        titanic = titanic.withColumn(f\"{vector_col}_{i}\", col(vector_col).getItem(i))\n",
    "    \n",
    "    titanic = titanic.drop(vector_col)\n",
    "\n",
    "# Save the processed data to GCS\n",
    "titanic.write.parquet(args.processed_data_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom training job\n",
    "In this section, we will create our custom training job. It will consist of the following steps:\n",
    "1. Create a Google Artifact Registry repository to host our custom container image.\n",
    "2. Create our custom training script.\n",
    "3. Create a Dockerfile that will specify how to build our custom container image. \n",
    "4. Build our custom container image.\n",
    "5. Push our custom container image to Google Artifact Registry so that we can use it in subsequent steps in our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "399eba3ab133"
   },
   "source": [
    "## Define the code for our training job\n",
    "\n",
    "The following code will create a file that contains the code for our custom training job. \n",
    "\n",
    "The code performs the following processing steps:\n",
    "\n",
    "1. Imports required libraries and sets initial variable values based on arguments passed to the script (the arguments are described below).\n",
    "2. Reads in the processed dataset that was created by the data preprocessing step in our pipeline.\n",
    "3. Fills in missing values in input features.\n",
    "4. Performs some feature engineering by creating new features such as 'FamilySize' and 'IsAlone' from combinations of existing features.\n",
    "5. Ensures that all numeric features are on a consistent scale with each other.\n",
    "6. One-hot encodes all categorical features.\n",
    "7. Converts the resulting sparse vector to a dense vector. This mainly makes it easier for us to feed the data into our Keras model in our training script later, with minimal processing needed in the training script.\n",
    "8. Writes the resulting processed data to a parquet file in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_DIR}/pyspark-model.py\n",
    "\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def train_model(args):\n",
    "    # Input arguments\n",
    "    processed_data_path = args.processed_data_path\n",
    "    model_path = args.model_path\n",
    "    \n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder.appName(\"TitanicSurvivalPrediction\").getOrCreate()\n",
    "\n",
    "    ### DATA PREPARATION SECTION ###\n",
    "    \n",
    "    # Read Parquet files into a DataFrame\n",
    "    data = spark.read.parquet(processed_data_path)\n",
    "\n",
    "    print(f\"Data loaded successfully from {processed_data_path}\")\n",
    "\n",
    "    # Separate the target and input features in the dataset\n",
    "    data = data.withColumnRenamed('Survived', 'label')\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    ### MODEL TRAINING AND EVALUATION SECTION ###\n",
    "\n",
    "    # Define the pipeline stages\n",
    "    assembler = VectorAssembler(inputCols=[col for col in data.columns if col != 'label'], outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
    "\n",
    "    # Construct the pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "    # Fit the pipeline to training data\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    print(f\"AUC: {auc}\")\n",
    "\n",
    "    # Save the model locally\n",
    "    model.write().overwrite().save(model_path)\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train a logistic regression model for Titanic survival prediction')\n",
    "\n",
    "    parser.add_argument('--processed_data_path', type=str, help='Path to the directory containing the preprocessed data')\n",
    "    parser.add_argument('--model_path', type=str, help='Path to save the trained model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload source code for PySpark\n",
    "\n",
    "We need to upload our PySpark code to our GCS bucket to be referenced by the `DataprocPySparkBatchOp` components in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil cp $PROCESSING_DIR/preprocessing.py $PREPROCESSING_PYTHON_FILE_URI\n",
    "! gsutil cp $TRAINER_DIR/pyspark-model.py $TRAINING_PYTHON_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQd9M1_9bif7"
   },
   "source": [
    "# Define our Vertex AI Pipeline\n",
    "\n",
    "Now that we have defined our custom data preprocessing and model training components, it's time to define our MLOps pipeline.\n",
    "\n",
    "In this section, we will use the Kubeflow Pipelines SDK and Google Cloud Pipeline Components to define our MLOps pipeline.\n",
    "\n",
    "We begin by specifying all of the required variables in our pipeline, and populating their values from the constants we defined earlier in our notebook. We then specify the following components in our pipeline:\n",
    "\n",
    "1. [DataprocPySparkBatchOp](https://cloud.google.com/vertex-ai/docs/pipelines/dataproc-component) to perform our data preprocessing step.\n",
    "2. [CustomTrainingJobOp](https://cloud.google.com/vertex-ai/docs/pipelines/customjob-component#customjobop) to perform our custom model training step.\n",
    "3. [importer](https://www.kubeflow.org/docs/components/pipelines/v2/components/importer-component/) to import our [UnmanagedContainerModel](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.UnmanagedContainerModel) object.\n",
    "4. [ModelUploadOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/model.html#v1.model.ModelUploadOp) to upload our Model artifact into Vertex AI Model Registry.\n",
    "5. [EndpointCreateOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.EndpointCreateOp) to create a Vertex AI [Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints).\n",
    "6. [ModelDeployOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/v1/endpoint.html#v1.endpoint.ModelDeployOp) to deploy our Google Cloud Vertex AI Model to an Endpoint, creating a [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) object within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX2u9hXDWtpp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME, description=\"MLOps pipeline for custom data preprocessing, model training, and deployment.\")\n",
    "def pipeline(\n",
    "    bucket_name: str = BUCKET_URI,\n",
    "    display_name: str = PIPELINE_NAME,\n",
    "    preprocessing_main_python_file_uri: str = PREPROCESSING_PYTHON_FILE_URI,\n",
    "    training_main_python_file_uri: str = TRAINING_PYTHON_FILE_URI,\n",
    "    preprocessing_args: list = PREPROCESSING_ARGS,\n",
    "    training_args: list = TRAINING_ARGS,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    subnetwork_uri: str = SUBNETWORK_URI,\n",
    "    dataproc_runtime_version: str = DATAPROC_RUNTIME_VERSION,\n",
    "    base_output_directory: str = PIPELINE_ROOT,\n",
    "):\n",
    "    \n",
    "    # Preprocess data\n",
    "    preprocessing_op = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        main_python_file_uri=preprocessing_main_python_file_uri,\n",
    "        args=preprocessing_args,\n",
    "        subnetwork_uri=subnetwork_uri,\n",
    "        runtime_config_version=dataproc_runtime_version,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    training_op = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        main_python_file_uri=training_main_python_file_uri,\n",
    "        args=training_args,\n",
    "        subnetwork_uri=subnetwork_uri,\n",
    "        runtime_config_version=dataproc_runtime_version,\n",
    "    ).after(preprocessing_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ePytY8t3bu"
   },
   "source": [
    "### Compile our pipeline into a YAML file\n",
    "\n",
    "Now that we have defined out pipeline structure, we need to compile it into YAML format in order to run it in Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFyCaNPIWtsU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, 'mlops-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq26zYhQb0qm"
   },
   "source": [
    "## Submit and run our pipeline in Vertex AI Pipelines\n",
    "\n",
    "Now we're ready to use the Vertex AI Python SDK to submit and run our pipeline in Vertex AI Pipelines.\n",
    "\n",
    "The parameters, artifacts, and metrics produced from the pipeline run are automatically captured into Vertex AI Experiments as an experiment run. We will discuss the concept of Vertex AI Experiments in more detail in laer chapters in the book. The output of the following cell will provide a link at which you can watch your pipeline as it progresses through each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwzYIoEabwRx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = aiplatform.PipelineJob(display_name=PIPELINE_NAME, template_path='mlops-pipeline.yaml', enable_caching=False)\n",
    "\n",
    "pipeline.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e428aab1826"
   },
   "source": [
    "### Wait for the pipeline to complete\n",
    "The following function will periodically print the status of our pipeline execution. If all goes to plan, you will eventually see a message saying \"PipelineJob run completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b05ed7a9cf3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job!! You have officially created and implemented an MLOps pipeline on Vertex AI!!\n",
    "\n",
    "I also highly recommend going through [this tutorial](https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml) to learn more about using Dataproc, BigQuery, and Apache Spark ML for Machine Learning! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1169ce7bd4c8"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UovhFyLeelQe"
   },
   "outputs": [],
   "source": [
    "clean_up = False\n",
    "\n",
    "if clean_up == True:\n",
    "    # Delete pipeline\n",
    "    pipeline.delete()\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_dataproc_tabular.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
