{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60c0fee-3e4f-46da-bd7d-a7e82098c8e4",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning / Optimization in Vertex AI\n",
    "In this notebook, we will use Vertex AI Vizier to perform hyperparameter tuning/optimization in Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "**Note:** This notebook and repository are supporting artifacts for the \"Google Machine Learning and Generative AI for Solutions Architects\" book. The book describes the concepts associated with this notebook, and for some of the activities, the book contains instructions that should be performed before running the steps in the notebooks. Each top-level folder in this repo is associated with a chapter in the book. Please ensure that you have read the relevant chapter sections before performing the activities in this notebook.\n",
    "\n",
    "**There are also important generic prerequisite steps outlined [here](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Prerequisite-steps/Prerequisites.ipynb).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b676c-f315-4489-b5b3-c593d32aa187",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd39e7-ef4b-4c9e-8577-b194c862034b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade xgboost google-cloud-aiplatform --user -q --no-warn-script-location"
   ]
  },
{
   "cell_type": "markdown",
   "id": "e5092e06-64ed-485f-9e24-e1198271b83f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "\n",
    "The code in the next cell will retart the kernel, which is sometimes required after installing/upgrading packages.\n",
    "\n",
    "**When prompted, click OK to restart the kernel.**\n",
    "\n",
    "The sleep command simply prevents further cells from executing before the kernel restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefcdf0-4ef3-4895-8700-347f8cdeceef",
   "metadata": {},
   "source": [
    "# (Wait for kernel to restart before proceeding...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c534-09a1-49e1-a6a5-3fd1b0417081",
   "metadata": {},
   "source": [
    "## Set Google Cloud resource variables\n",
    "\n",
    "The following code will set variables specific to your Google Cloud resources that will be used in this notebook, such as the Project ID, Region, and GCS Bucket.\n",
    "\n",
    "**Note: This notebook is intended to execute in a Vertex AI Workbench Notebook, in which case the API calls issued in this notebook are authenticated according to the permissions (e.g., service account) assigned to the Vertex AI Workbench Notebook.**\n",
    "\n",
    "We will use the `gcloud` command to get the Project ID details from the local Google Cloud project, and assign the results to the PROJECT_ID variable. If, for any reason, PROJECT_ID is not set, you can set it manually or change it, if preferred.\n",
    "\n",
    "We also use a default bucket name for most of the examples and activities in this book, which has the format: `{PROJECT_ID}-aiml-sa-bucket`. You can change the bucket name if preferred.\n",
    "\n",
    "Also, we're defaulting to the **us-central1** region, but you can optionally replace this with your [preferred region](https://cloud.google.com/about/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6de65-79e8-46ba-a1e8-2959d22f96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID_DETAILS = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID_DETAILS[0]  # The project ID is item 0 in the list returned by the gcloud command\n",
    "BUCKET=f\"{PROJECT_ID}-aiml-sa-bucket\" # Optional: replace with your preferred bucket name, which must be a unique name.\n",
    "REGION=\"us-central1\" # Optional: replace with your preferred region (See: https://cloud.google.com/about/locations) \n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Bucket Name: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868271aa-3ec1-45d7-be9d-c36aef4ef395",
   "metadata": {},
   "source": [
    "## Create bucket\n",
    "\n",
    "The following code will create the bucket if it doesn't already exist.\n",
    "\n",
    "If you get an error saying that it already exists, that's fine, you can ignore it and continue with the rest of the steps, unless you want to use a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2de5e-c2ef-40a1-a49e-9614f1107b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l us-central1 gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1592827-4f0c-442c-beb7-04e18f2bb58e",
   "metadata": {},
   "source": [
    "# Begin implementation\n",
    "\n",
    "Now that we have performed the prerequisite steps for this activity, it's time to implement the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1592827-4f0c-442c-beb7-04e1022bb58e",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Using Vertex AI Vizier for hyperparameter tuning involves several steps. First, we'll need to create a training application, which will consists of a Python script that trains our model with given hyperparameters and then saves the trained model. This script must also report the performance of the model on the validation set, so Vertex AI Vizier can determine the best hyperparameters.\n",
    "\n",
    "Next, we need to create a configuration file for the hyperparameter tuning job, which specifies the hyperparameters to tune and their possible values, as well as the metric to optimize.\n",
    "\n",
    "Finally, we'll use the Vertex AI Vizier client library to submit the hyperparameter tuning job, which will run our training application with different sets of hyperparameters, and find the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0e2eb-d8a5-42fe-820e-9a158e01b811",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a8002-7b02-4356-803b-611710b10082",
   "metadata": {},
   "source": [
    "Set additional variables related to our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd487e9-cfb9-4cc1-8e87-0c0d6608626c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI=f\"gs://{BUCKET}\"\n",
    "APP_NAME=\"fraud-detect\"\n",
    "APPLICATION_DIR = \"vizier\"\n",
    "TRAINER_DIR = f\"{APPLICATION_DIR}/trainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ede06-29f5-4685-800b-bc7bb5b52a68",
   "metadata": {},
   "source": [
    "## Get the source data for this use case\n",
    "\n",
    "1. Download the \"Credit Card Fraud Detection\" dataset from [this link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3). \n",
    "2. The file downloads as a zip archive, so you will need to extract the creditcard.csv from within that zip archive.\n",
    "3. In the top-left corner your JupyterLab screen – i.e., the screen on which you are currently reading these instructions – click upload symbol (the symbol is an arrow pointing upwards).\n",
    "4. Upload the creditcard.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b507d87-24b8-40b0-bb09-415f5b14f24e",
   "metadata": {},
   "source": [
    "## Clean the data and transfer it to GCS\n",
    "\n",
    "The following code removed non-numeric values from the target variable in our dataset.\n",
    "\n",
    "The gsutil command then transfers the data to GCS to be referenced in our training script later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c391a7b-53af-410d-8f3c-956e3f2af5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'creditcard.csv'  \n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Check for unique values in 'Class'\n",
    "print(df['Class'].unique())\n",
    "\n",
    "# Optionally, replace or drop rows with unexpected values\n",
    "df.dropna(subset=['Class'], inplace=True)  # Drop rows with NaN in 'Class'\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file, overwriting the original\n",
    "df.to_csv(file_path, index=False)  # index=False to avoid writing row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66c23e-f81b-4251-b011-74437682799e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp creditcard.csv $BUCKET_URI/creditcard.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76022d02-610a-40ec-9225-d7a3690a4d29",
   "metadata": {},
   "source": [
    "## Containerize the training application code\n",
    "\n",
    "Before we can run a hyperparameter tuning job, we need to create a source code file (training script) and a Dockerfile. The source code trains a model using XGBoost, and the Dockerfile will include all the commands needed to run the container image.\n",
    "\n",
    "It will install all of the libraries required by our training script, and set up the entry point for the training code.\n",
    "\n",
    "First, let's create a couple of directories that we'll use, and import and initialize the Google Cloud AI Platfrom client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2206f-d695-4507-b648-0ab2c5059d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p $APPLICATION_DIR\n",
    "!mkdir -p $TRAINER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b303785-8351-497f-8041-70011174d9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b7a18-1555-43c1-9499-219ac7ab9b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the AI Platform client\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58e964-2b26-4767-95fd-3fdd6cd5678a",
   "metadata": {},
   "source": [
    "### Create the training application (train.py)\n",
    "\n",
    "The code in the next cell will create our training script.\n",
    "\n",
    "**Important notes for our training code:**\n",
    "\n",
    "*Notes related to XGBoost:*\n",
    "\n",
    "* DMatrix is a data structure used by XGBoost that is optimized for both memory efficiency and training speed. We will convert our training, validation, and test datasets into DMatrix format before training the model.\n",
    "\n",
    "* The param dictionary contains the parameters for the XGBoost model. eta is the learning rate, max_depth is the maximum depth of the trees, objective is the loss function to be minimized, and random_state is a seed for the random number generator for reproducibility.\n",
    "\n",
    "* num_round is the number of rounds of training, equivalent to the number of trees in the model.\n",
    "\n",
    "* The train function trains the model, and the predict function generates predictions. The predictions are probabilities of the positive class (fraudulent transactions), so they are between 0 and 1. We can convert these to class labels (0 or 1) by rounding them to the nearest integer (in reality, we could choose a different threshold depending on the business requirements).\n",
    "\n",
    "*Notes related to training and tuning with Vertex AI Vizier:*\n",
    "\n",
    "* We use the [cloudml-hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) Python package to pass metrics to Vertex AI. To learn more about this process, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/code-requirements#hp-tuning-metric).\n",
    "\n",
    "* For hyperparameter tuning, Vertex AI runs our training code multiple times, with different command-line arguments each time. Our training code must parse these command-line arguments and use them as hyperparameters for training.. To learn more about this process, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/code-requirements#command-line-arguments).\n",
    "\n",
    "**IMPORTANT:** Replace **YOUR_BUCKET_NAME** with your bucket name. \n",
    "This is because *writefile* will write the contents of this cell directly to file; it will not parse variables from earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c66db-6f72-4a9b-9596-b0187c396153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_DIR}/train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import storage\n",
    "from hypertune import HyperTune\n",
    "\n",
    "data_location='gs://YOUR_BUCKET_NAME/creditcard.csv'\n",
    "\n",
    "def train_model(data, max_depth, eta, gamma):\n",
    "    data.dropna(subset=[data.columns[-1]], inplace=True)\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Split the non-training data into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "        \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'max_depth': max_depth,\n",
    "        'eta': eta,\n",
    "        'gamma': gamma,\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    num_round = 10\n",
    "    model = xgb.train(params, dtrain, num_round, evallist)\n",
    "    \n",
    "    preds = model.predict(dtest)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    hpt = HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='auc',\n",
    "        metric_value=auc,\n",
    "        global_step=1000)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='XGBoost Hyperparameter Tuning')\n",
    "    parser.add_argument('--max_depth', type=int, default=3)\n",
    "    parser.add_argument('--eta', type=float, default=0.3)\n",
    "    parser.add_argument('--gamma', type=float, default=0)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    data = pd.read_csv(data_location)\n",
    "    model = train_model(data, args.max_depth, args.eta, args.gamma)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8050141-8b58-4566-8d1d-5b42c9a836b1",
   "metadata": {},
   "source": [
    "### Create our requirements.txt file\n",
    "The requirements.txt file is a convenient way to specify all of the packages that we want to install in our custom container image. This file will be referenced in the Dockerfile for our image.\n",
    "\n",
    "In this case, we will install:\n",
    "* [XGBoost](https://xgboost.readthedocs.io/en/stable/)\n",
    "* [cloudml-hypertune 0.1.0.dev6](https://pypi.org/project/cloudml-hypertune/)\n",
    "* [The Vertex AI Python SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "* [Python Client for Google Cloud Storage](https://cloud.google.com/python/docs/reference/storage/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e733c1-8987-40bb-9e32-319af45361ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/requirements.txt\n",
    "xgboost\n",
    "cloudml-hypertune==0.1.0.dev6\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92bd89-640c-4d41-a20f-5f2c18262904",
   "metadata": {},
   "source": [
    "### Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use a Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381653d-be05-449b-aa8b-7241e1791027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {APPLICATION_DIR}/Dockerfile\n",
    "\n",
    "# Use a Vertex AI prebuilt container for custom training as a base image.\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest\n",
    "\n",
    "# Specify the working directory to use in our container\n",
    "WORKDIR /\n",
    "\n",
    "# Copy our requirements.txt file to our container image\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install the packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the training code to our container image\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke our training code.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32ae9c-7328-4e48-b8af-3e3ad992d514",
   "metadata": {},
   "source": [
    "### Build the container and put it in Google Artifact Registry\n",
    "Next, we'll create a Docker repository in Google Artifact Registry, build our container, and push it to the newly-created repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8229fa1-2076-4fab-9744-8705ca703397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_NAME=f'{APP_NAME}-app'\n",
    "\n",
    "!gcloud artifacts repositories create $REPO_NAME --repository-format=docker \\\n",
    "--location=$REGION --description=\"Docker repository\"\n",
    "\n",
    "! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cd6f6-48dd-4f12-ad5e-66b565574d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_URI = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{APP_NAME}:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c288bac-0f0e-4609-afc9-cf826e727b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd $APPLICATION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cc0fd-e96d-4fd7-bad8-c222b7cbf664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker build ./ -t $IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3ad35-0242-4fc6-a43b-0f2508abeb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417826a-11dd-46fb-941b-cc45d926db4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42785616-dddf-40cd-beb3-b111f0d24dfa",
   "metadata": {},
   "source": [
    "## Configure a hyperparameter tuning job\n",
    "Now that our training application code is containerized, it's time to specify and run the hyperparameter tuning job.\n",
    "\n",
    "To create the hyperparameter tuning job, we need to first define the worker_pool_specs, which specifies the machine type and Docker image to use. The following spec includes one n1-standard-4 machine. (For more details, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec3c29-c5d5-4b3d-a75b-75c6e6a45408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The spec for the worker pools, including machine type and Docker image\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36cc21-9a99-4783-bad0-d94da3739622",
   "metadata": {},
   "source": [
    "## Define our custom job spec and hyperparameter tuning spec.\n",
    "Next, we define our custom job spec (referencing the worker pool specs we just created), and our hyperparameter tuning spec, which includes details such as the hyperparameters and the metrics we want to optimize. (For more details, see the Google Cloud documentation [here](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#aiplatform_create_hyperparameter_tuning_job_python_package_sample-python).)\n",
    "\n",
    "**IMPORTANT:** If you named your service account anything other than **ai-ml-sa** when you created it at the beginning of Chapter 8 then you will need to replace it in this code cell. (If you followed the recommended naming then you do not need to make a change here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b013de7-8b55-4e1f-98c3-5018ba011ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom job\n",
    "custom_job = aiplatform.CustomJob(\n",
    "    display_name=\"xgboost_train\",\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n",
    "\n",
    "# Specify service account\n",
    "service_account_email = f\"ai-ml-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# Set the custom service account in the job config\n",
    "custom_job.service_account_email = service_account_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778e182-acac-47e3-865f-37e338234676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter tuning spec\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"xgboost_hpt\",\n",
    "    custom_job=custom_job,\n",
    "    metric_spec={\n",
    "        \"auc\": \"maximize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"eta\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.01, max=0.3, scale='unit'),\n",
    "        \"max_depth\": aiplatform.hyperparameter_tuning.IntegerParameterSpec(min=3, max=10, scale='unit'),\n",
    "        \"gamma\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0, max=1, scale='unit'),\n",
    "    },\n",
    "    max_trial_count=20,\n",
    "    parallel_trial_count=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3439a-84af-4871-8552-481280a80e8b",
   "metadata": {},
   "source": [
    "# Run the hyperparameter tuning job\n",
    "\n",
    "The following cell will run our job. Considering that the tuning job will include many trials, it may run for a long time (perhaps an hour or two). The output of this cell will display a link that will enable you to view the status of the tuning job in the Google Cloud console. The output of this cell will also repetitively display the current status of the tuning job every few seconds here in this notebook. Wait until the current status says \"JOB_STATE_SUCCEEDED HyperparameterTuningJob run completed\", and then we will inspect the optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3774b-5abf-41f8-a6ff-c839eec08d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the hyperparameter tuning job\n",
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710f7eb-a920-45c0-a567-65980509c028",
   "metadata": {},
   "source": [
    "# Extract the best hyperparameters\n",
    "\n",
    "In the next cell, we will get a list of all of the trials from our tuning job, then find the best-performing trial, and extract its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084a649-049c-4173-b5fb-6b38a5d48379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of trials sorted by the objective metric (auc) in descending order\n",
    "trials = sorted(hpt_job.trials, key=lambda trial: trial.final_measurement.metrics[0].value, reverse=True)\n",
    "\n",
    "# The first trial in the sorted list is the best trial\n",
    "best_trial = trials[0]\n",
    "best_auc = trials[0].final_measurement.metrics[0].value\n",
    "\n",
    "# Extract hyperparameters of the best trial\n",
    "best_hyperparameters = best_trial.parameters\n",
    "\n",
    "print(f\"Best AUC: {best_auc}\")\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87223a-ffcb-4367-a0a7-17a004fbd537",
   "metadata": {},
   "source": [
    "# Train a model with the best hyperparameters\n",
    "\n",
    "Now, let's train a model with the best hyperparameters that were produced by our tuning job.\n",
    "\n",
    "Note that **num_boost_round** is not a parameter of the model, but rather a parameter of the training function, so we will handle it separately. We will also convert it to an integer type here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ceeb2-cd5c-4b6c-beda-12e91f1a50a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "for param in best_hyperparameters:\n",
    "    param_id = param.parameter_id\n",
    "    if param_id == 'num_boost_round':\n",
    "        best_params[param_id] = int(param.value)\n",
    "    else:\n",
    "        best_params[param_id] = param.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13956b3-4fb4-4d5d-8cec-9a2683da3aa1",
   "metadata": {},
   "source": [
    "## Install XGboost\n",
    "\n",
    "Let's install XGBoost so we can train a model directly here in our notebook (remember that our previous training jobs happened in a Docker container that we had created.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47da29d-5c0e-456f-a5ea-03ab0fccc3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0250d54-8680-4902-949d-554a282059c2",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We will use a modified version of our earlier training code. In this case, we will directly provide the \"best_params\" to the training job.\n",
    "\n",
    "The ouput of this cell will show us the ROC-AUC score achieved against the **validation** dataset for each training round (specified by num_round).\n",
    "\n",
    "Finally, we will evaluate our model against the **test** dataset, and print the resulting ROC-AUC score for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ade40f-3720-4efa-a700-2fbd8809bb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_location=f'{BUCKET_URI}/creditcard.csv'\n",
    "\n",
    "def train_model(data, hyperparameters):\n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Split the non-training data into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "        \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Convert max_depth to int (xgboost expects it as an int)\n",
    "    hyperparameters['max_depth'] = int(hyperparameters['max_depth'])\n",
    "\n",
    "    hyperparameters.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 4,\n",
    "        'eval_metric': 'auc'\n",
    "    })\n",
    "    \n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    num_round = 10\n",
    "    model = xgb.train(hyperparameters, dtrain, num_round, evals=evallist)\n",
    "    \n",
    "    preds = model.predict(dtest)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    print(f'ROC-AUC Score on Test Set: {auc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(data_location)\n",
    "    model = train_model(data, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde69502-86be-4ba8-b190-2a616189ae81",
   "metadata": {},
   "source": [
    "**Note:** when I ran this, I got an ROC-AUC score of 0.9188, which is pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabaae8-0dfc-4fd5-97a4-959944a7fc72",
   "metadata": {},
   "source": [
    "# That's it! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb76c1-7a07-4d89-ada7-21245c7be5ba",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "When you no longer need the resources created by this notebook. You can delete them as follows.\n",
    "\n",
    "**Note: if you do not delete the resources, you will continue to pay for them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefbb9b-3110-4a1f-8e5b-6655568efd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_up = False  # Set to True if you want to delete the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74595a-473b-4894-8b43-840032bc1e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete Vizier resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab384da3-1882-46b3-9b3e-deef73ebf5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if clean_up:\n",
    "    # Delete HPT job\n",
    "    try:\n",
    "        hpt_job.delete()\n",
    "        print(\"Deleted HPT job successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting HPT job: {e}\")\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565a95b-ce8d-40b2-8c88-62bbcf479e43",
   "metadata": {},
   "source": [
    "## Delete artifact repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87485c0c-d064-4e69-9dd4-cb7968a05ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_up == True:\n",
    "    # Delete the Artifact repository\n",
    "    ! gcloud artifacts repositories delete $REPO_NAME --location=$REGION --quiet\n",
    "else:\n",
    "    print(\"clean_up parameter is set to False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2f8a5-4748-4be8-bc1e-f5007f00d471",
   "metadata": {},
   "source": [
    "## Delete GCS Bucket\n",
    "The bucket can be reused throughout multiple activities in the book. Sometimes, activities in certain chapters make use of artifacts from previous chapters that are stored in the GCS bucket.\n",
    "\n",
    "I highly recommend **not deleting the bucket** unless you will be performing no further activities in the book. For this reason, there's a separate `delete_bucket` variable to specify if you want to delete the bucket.\n",
    "\n",
    "If you want to delete the bucket, set the `delete_bucket` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2e150-d910-415e-add3-b8274ee766da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca201-3eaa-4e13-b92a-6b8902df7ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if delete_bucket == True:\n",
    "    # Delete the bucket\n",
    "    ! gcloud storage rm --recursive gs://$BUCKET\n",
    "else:\n",
    "    print(\"delete_bucket parameter is set to False\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
